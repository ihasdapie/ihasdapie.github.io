<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta lang="en">

    <title>Engsci Year 2 Winter | chenbrian</title>

    <style>*{box-sizing:border-box}body{margin:40px auto;max-width:800px;line-height:1.6;font-size:16px;color:#444;font-family:sans-serif;padding:0 10px}h1,h2,h3{line-height:1.2;font-family:sans-serif;color:#444}div.header h1{padding-top:0;padding-bottom:8px;margin-bottom:24px;font-size:16px;font-weight:400;border-bottom:1px solid}.header-menu{float:right}ul.pagination{list-style-type:none;text-align:center;padding:0}ul.pagination>li{padding:0 8px;display:inline-block}div.footer{border-top:1px solid;text-align:center}img{max-width:100%;max-height:100%;display:block;margin-left:auto;margin-right:auto}button{border:none;background:0 0;border-radius:5px}button:hover{background:#f1f1f1}table{border-collapse:collapse;width:100%}td,th{border:1px solid #444;padding:.5rem;text-align:left}@media only screen and (max-width:1450px){#back_to_top{visibility:visible!important}#toc{display:none}}.katex-display>.katex{overflow:auto hidden}.katex .katex-html>.newline{display:block;padding-bottom:.25em;padding-top:.25em}#toc{position:fixed;top:40px;right:0;padding-right:80px;line-height:1.4em;max-width:400px;margin:auto}#back_to_top{position:fixed;top:10px;right:10px;width:100px;visibility:hidden}blockquote{background:#f1f1f1;border-left:4px solid #ccc;padding-left:8px}code:not([class*=language-]){border-radius:.25rem;padding:.1rem;background-color:#f1f1f1}p+ul,p+ol,ul+ol,ol+ul{margin-top:-10px}ul,ol,li{padding-left:.5 em}a.footnote-ref::before{content:'['}a.footnote-ref::after{content:']'} 
            body {
                margin-left: 250px;   
            }
            @media only screen and (max-width: 1450px) {
                body {
                    margin: 40px auto;
                }
            }

        





    </style>



    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B0ZFV4LTG2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-B0ZFV4LTG2', { 'anonymize_ip': false });
}
</script>


    

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#33b884">
<meta name="msapplication-TileColor" content="#33b884">
<meta name="theme-color" content="#33b884">



</head>

<body>
<div class="header">
    <h1>
        <a href="//chenbrian.ca">chenbrian</a>
        <div class="header-menu">
            <a href="/about/">about</a> &nbsp;
            <a href="/posts/">blog</a> &nbsp;
            <a href="/tags/">tags</a> &nbsp;
        </div>
    </h1>
</div>

<div id="content">
        
            <aside id="back_to_top">
            <a href="#">Back to top</a>
            </aside>
        
        <section>
            
                <div id="toc">
    <h4>Table of Contents</h4>
    <aside>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#ece286-probability-and-statistics">ECE286: Probability and Statistics</a>
      <ul>
        <li><a href="#probability">Probability</a></li>
        <li><a href="#bayes-rule--random-variables">Bayes' Rule &amp; Random Variables</a></li>
        <li><a href="#distributions">Distributions</a></li>
      </ul>
    </li>
    <li><a href="#ece259-electromagnetism">ECE259: Electromagnetism</a>
      <ul>
        <li><a href="#vector-calculus-review">Vector Calculus Review</a></li>
        <li><a href="#electric-fields">Electric Fields</a></li>
      </ul>
    </li>
    <li><a href="#bme205-introduction-to-biomedical-engineering">BME205: Introduction to Biomedical Engineering</a>
      <ul>
        <li><a href="#cells">Cells</a></li>
      </ul>
    </li>
    <li><a href="#phy294-quantum-and-thermal-physics">PHY294: Quantum and Thermal Physics</a>
      <ul>
        <li><a href="#schrodinger--the-hydrogen-atom">Schrodinger &amp; the Hydrogen Atom</a></li>
        <li><a href="#electron-spin">Electron Spin</a></li>
        <li><a href="#multi-electron-atoms-pauli-principle-and-periodic-table">Multi-Electron Atoms, Pauli Principle, and Periodic Table</a></li>
        <li><a href="#molecules-and-bonding">Molecules and Bonding</a></li>
      </ul>
    </li>
    <li><a href="#esc204-praxis-iii">ESC204: Praxis III</a></li>
    <li><a href="#tep327-engineering-and-law">TEP327: Engineering and Law</a>
      <ul>
        <li><a href="#case-briefing">Case Briefing</a></li>
        <li><a href="#resources">Resources</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </aside>
</div>





            
        <header>
    <h1>Engsci Year 2 Winter</h1>
    





<div class="post-meta">
    Date:
    <time datetime="2022-01-10">Jan 10, 2022</time>,&nbsp;Last Modified:
    <time datetime="2022-03-01">Mar 1, 2022</time>
    <br>
    
    Tags &#x5b;
    <a href="//chenbrian.ca/tags/notes/">notes</a>
    <a href="//chenbrian.ca/tags/engsci/">engsci</a>
    &#x5d;
</div>

</header>
<article>
    <p>// work in progress //</p>
<p><object data="notes.pdf#toolbar=0&navpanes=0" type="application/pdf" width=100% height="700px">
    <embed src="notes.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="notes.pdf">Download PDF</a>.</p>
    </object>
</object>







Or, download <a href="notes.pdf">here</a></p>
<p>Notes prior to reading week can be found below.
Right before reading week my roommate introduced me to the <a href="https://github.com/Adhumunt/NotesTeX">NotesTeX</a> template and I&rsquo;m currently using that.</p>
<h2 id="ece286-probability-and-statistics">ECE286: Probability and Statistics</h2>
<h3 id="probability">Probability</h3>
<h4 id="coin-flip-example--combining-probabilities">Coin flip example &amp; combining probabilities</h4>
<p>Outcomes denoted by variables, i.e. for coin clip \( H \), \( T \). Probability of outcome is denoted by \( P(X) \), where \( X \) is the outcome. We must have \( \sum(P) = 1 \) since \( 100% \).</p>
<p>Suppose \( P(H) = 0.3 \) , \( P(T) = 0.7 \).</p>
<p>Then, $$P(HT) = P(H)P(T) = 0.3 \times 0.7 = 0.21 $$
$$ P(HT) = P(H)P(T) = 0.3 \times 0.7 = 0.21 $$
$$ P(HT \text{ or  } TH) = P(H)P(T) + P(T)P(H) = \\ 0.3 \times 0.7 + 0.7 \times 0.3 = 0.42 $$</p>
<h4 id="sets-and-events">Sets and events</h4>
<p><strong>Sample space</strong>: set of all possible outcomes.
E.x.</p>
<ul>
<li>coin flip: \( S = {H, T} \)</li>
<li>roll of a die: \( S = {1,2,3,4,5,6} \)</li>
<li>roll of a die but only care even/odd: \( S = {even, odd} \)</li>
</ul>
<p><strong>Event</strong>: a subset of sample space</p>
<p>For example, each element, \(e \in S = {1,2,3,4,5,6} \) are the elements of S for a die.</p>
<p><strong>Complement of an event A w.r.t. S</strong>: everything in \( S \) and not \( A \), denoted \( A' \).<br>
Example: for a die, \( {1, 2} \) is the complement of \( {3,4,5,6} \)</p>
<p><strong>Intersection of two events</strong>: everything in \( A \) <em>and</em> \( B \), denoted \( A \cap B \)</p>
<p><strong>Union of two events</strong>: everything in \( A \) <em>or</em> \( B \), denoted \( A \cup B \)</p>
<h4 id="counting">Counting</h4>
<p><strong>Multiplication Rule</strong>:
The total amount of outcomes is the product of the amount of outcomes in each event.
I.e. if for a sequence if \( k \) events \( E_1, E_2, &hellip;, E_k \) with outcomes \( S_1, S_2, &hellip;, S_k \)
then the total amount of outcomes is \( \prod_{i=1}^k S_i \)</p>
<p><strong>Permutations</strong>: \( n \) distinct objects can be arranged in \( n! \) ways. When considering permuting a subset of size \( r \) taken from a set of size \( n \), the total number of arrangements is given by</p>
<p>$$ nPr = \frac{n!}{(n-r)!} $$</p>
<p>If we can have repeated kinds, i.e. if there are \( m \) kinds of items and \( n_k, k = 1, \dots m \) of each kind, then there are</p>
<p>$$ \frac{n!}{n_1!n_2!\dots n_m!}$$ permutations.</p>
<p>For example, we can order &ldquo;ATLANTIC&rdquo; \( \frac{8!}{2!2!1!1!1!1!}  = 10080\) times.</p>
<p>The same formula applies for finding the number of ways to arrange \( n \) items into \( k \) subsets of size \( n_1, n_2, \dots , n_k \)</p>
<p><strong>Combinations</strong>: Permutations, but order doesn&rsquo;t matter.</p>
<p>$$ nCr = \binom{n}{r} = \frac{n!}{r!(n-r)!} $$</p>
<p>Note similarity to the partition formula; combinations can be thought as the set of partitions of size \( 1 \).</p>
<h4 id="additive-rules">Additive Rules</h4>
<p>For events A, B:
$$ P(A\cup B) = P(A) + P(B) - P(A\cap B)$$</p>
<p>For \(  n \) mutually exclusive events \( A_1, A_2 &hellip; A_n \)
$$ P(A_1 \cup A_2 \cup &hellip; A_n) = P(A_1) + P(A_2) + &hellip; + P(A_n)$$</p>
<p>And if \( A_1, A_2 &hellip; A_n \) is a partition of sample space S,</p>
<p>$$ P(A_1 \cup A_2 \cup &hellip; A_n) = P(A_1) + P(A_2) + &hellip; + P(A_n)$$</p>
<p>And:
$$ P(A \cup B) + P(A) + P(B) $$</p>
<h5 id="conditional-probability">Conditional Probability</h5>
<p>$$ P(B|A) = \frac{P(A\cap B}{P(A)}$$</p>
<blockquote>
<p>The probability of B occurring, given that A occurs</p>
</blockquote>
<h4 id="total-probability">Total probability</h4>
<p>Suppose A is an event and \(B_1, &hellip; B_k\) is a partition.</p>
<blockquote>
<p>Recall: \( B_1 &hellip; B_k \)) is a partition if \( B_i \cap B_j \neq 0\) and \( B_1 \cup &hellip; B_k = S\)</p>
</blockquote>
<p>$$ P(A) = \sum_{i=1}^k P(A \cap B_i) $$</p>
<p>For example, if we have machines \( B_1, B_2, B_3 \) that make products  30%, 45%, 25% of the time (note that this forms a partition)</p>
<p>And that they produce 2%, 3%, 2% defective products, we can find the probability of there being a defect as</p>
<p>$$ P(D) = P(B_1)P(D|B_1) + P(B_2)P(D|B_2) +P(B_3)P(D|B_3) \\
= (0.3 * 0.02) + (0.45 * 0.03) + (0.25 * 0.02) = 0.0245 $$</p>
<p>And we can get the chance it came from machine 2 by applying Bayes' rule.</p>
<h3 id="bayes-rule--random-variables">Bayes' Rule &amp; Random Variables</h3>
<hr>
<p><em>Define</em>: Bayes' Rule
$$ P(C_n|A) = \frac{P(C_n)P(A|C_n)}{\sum^k_{i=1}  P(C_i)P(A|C_i)} $$</p>
<hr>
<p>E.x. Probability that a defective product comes from machine 3 \( B_3 \)</p>
<p>$$P(B_3|D) = \frac{P(B_3)P(D|B_3)}{P(D)} = \frac{0.25*0.02}{0.0245} = 10/49$$</p>
<blockquote>
<p><em>Given</em> that the product is defective, what is the chance it came from machine 3?</p>
</blockquote>
<p>Bayes' rule is useful when we have limited information.
For example if we&rsquo;re trying to check if we have a disease:
Given:</p>
<ol>
<li>Chance of having disease \( P(D) \) = \( 1/1000 \)</li>
<li>A test kit that has a true positive rate \( P(T = 1|D) = 0.7 \), false positive \(P(T=1| not D) = 0.05\)</li>
</ol>
<blockquote>
<p>What is \(  P(D|T = 1) \)? (Chance of having disease given test kit is true positive?)</p>
</blockquote>
<p>$$ P(D|T=1) = \frac{P(T=1|D)P(D)}{P(T=1|D)P(D)+P(T=1|not D)P(not D) } \\
= (0.7 \times 0.001 ) / (0.7 \times 0.001 + 0.05 \times 0.999) \approx 0.014 $$</p>
<blockquote>
<p>This may look like a number that&rsquo;s really off. Why would a positive test result only
indicate a 1.4% change of having the disease?
Must recall: this as the &ldquo;inverse&rdquo; of the true positive rate (chance of testing positive if you have the disease).</p>
</blockquote>
<p>This can be applied in sequential state estimation.
Given</p>
<ul>
<li>State \( X_t \)</li>
<li>Observation \( Y_t = g(X_t) \)</li>
<li>Dynamics \( P(X_{t+1} | X_t) \) (typically Markov chain<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>/LTI (Linear Time-Invariant)<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> system)</li>
</ul>
<blockquote>
<p>Markov chains are a stochastic model where the probability of each state depends only on the previous state</p>
</blockquote>
<p>We can build an estimator&hellip;</p>
<p>$$ P(Y_{t+1}|X_{t+1}) = \frac{P(Y_{t+1}|X_{t+1},X_t)P(X_{t+1}|X_t)}{P(Y_{t+1})}$$</p>
<ul>
<li>Optimal solution to LTI system problem is the <a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a>. Often used for vehicle dynamics, circuits, or anything that requires a long look into the &ldquo;future&rdquo;</li>
<li>Example application of Markov chains could be a machine with a few states {active, inactive, broken}.
<ul>
<li>Markov chains can sometimes be directly computable &amp; are often central to DP/RL problems</li>
</ul>
</li>
</ul>
<h4 id="random-variables">Random Variables</h4>
<blockquote>
<p>A <strong>Random</strong> variable is a function that associates a real number with each element in the sample space</p>
</blockquote>
<p>E.x if we&rsquo;re testing some components and \( N \) denotes non-defective and \( D \) denotes defective,</p>
<p>$$ S = \{NNN, NND, NDN, DNN, NDD, DND, DDN, DDD\} $$</p>
<p>and we are concerned about the number of defective items,
the outcome (total no. of defective items) can be assumed by a <em>random variable</em>, let&rsquo;s call it \( X \), which can take on the values \( \{0, 1, 2, 3 \} \).
So each value of \(x \in X  \)  represents an event that is a subset of the sample space.
For example \(  x = 2 \) corresponds to a subset \( \{DDN, DND, NDD\} \).</p>
<ul>
<li>Discrete random variables take on finite/countable values; denote w/ capital letters</li>
<li>Continuous random variables take on values in an interval of \( \mathbb{R} \)
<ul>
<li>Has a probability of 0 of assuming <em>exactly</em> any of its values</li>
<li>Denote individual values with the lowercase letter equivalent</li>
</ul>
</li>
</ul>
<h3 id="distributions">Distributions</h3>
<h4 id="discrete-probability-distributions">Discrete Probability Distributions</h4>
<blockquote>
<p>Probability that a discrete RV takes on each value</p>
</blockquote>
<p>I.e. if \( X \) is the # of heads for 3 coin flips</p>
<ul>
<li>\( P(X=0) = 1/8 \)</li>
<li>\( P(X=1) = 3/8 \)</li>
<li>\( P(X=2) = 3/8 \)</li>
<li>\( P(X=3) = 1/8 \)</li>
</ul>
<h4 id="probability-functionmass-functiondistribution-pmf">Probability function/mass function/distribution (PMF)</h4>
<hr>
<p><em>Define</em>: \( f(x) \) is a PMF of the discrete RX \( X \) if \(  \forall x \in X \)</p>
<ol>
<li>\( f(x) \geq 0 \)</li>
<li>\( \sum_x f(x) = 1 \)</li>
<li>\( P(X=x) = f(x) \)</li>
</ol>
<hr>
<h4 id="cumulative-distribution-function">Cumulative Distribution Function</h4>
<blockquote>
<p>When we want to find if the observed value of a random variable will be less than or equal to some real number \( x \).</p>
</blockquote>
<hr>
<p><em>Define</em>: Cumulative Distribution Function (\( CDF \))</p>
<p>$$ F(x) = P(X \leq x) = \sum_{t\leq x} f(t), \hspace{1cm} - \infty &lt; x &lt; \infty $$</p>
<hr>
<blockquote>
<p>Note that this function is not only defined for the values assumed by the random variable, but for all real numbers as well.</p>
</blockquote>
<h4 id="continuous-probability-distributions">Continuous Probability Distributions</h4>
<blockquote>
<p>Recall: Continuous RV have a probability of 0 of assuming <em>exactly</em> any of its values.</p>
</blockquote>
<p>Consider a RV whose values are the lengths of hair on one&rsquo;s head. There are an infinite number of hair lengths between any two values, and as such we assign a probability of 0 to each event.
Instead we concern ourselves with probabilities across intervals.</p>
<hr>
<p><em>Define</em>: Probability Density Function (\( PDF \))</p>
<p>\( f(x) \) is a \( pdf \) for the continuous random variable \( X \) if</p>
<ol>
<li>\( f(x) \geq 0 \)</li>
<li>\( \int_{-\infty}^\infty f(x) = 1 \)</li>
<li>\( P(a &lt; X &lt; b)= \int_a^bf(x) \)</li>
</ol>
<hr>
<blockquote>
<p>We can apply the CDF concept to a continuous RV with a density function as well</p>
</blockquote>
<hr>
<p><em>Define</em>: Cumulative Distribution Function (\( CDF \))</p>
<p>$$ F(x) = P(X \leq x) = \int_{-\infty}^x f(t) dt, \hspace{1cm} - \infty &lt; x &lt; \infty $$</p>
<hr>
<p>A nice result can then be obtained by applying the fundamental theorem of calculus,</p>
<p>$$ P(a &lt; X &lt; b) = F(b) - F(a) $$</p>
<p>and</p>
<p>$$ f(x) = \frac{dF(x)}{dx} $$</p>
<blockquote>
<p>NOTE: Probability <em>mass</em> functions describe the probability of <em>discrete</em> distributions, whereas probability <em>density</em> functions describe <em>continuous</em> distributions.</p>
</blockquote>
<h4 id="joint-probability-distributions">Joint Probability Distributions</h4>
<blockquote>
<p>What if we want to deal with more than one random variable at a time?</p>
</blockquote>
<hr>
<p><em>Define</em>: \( f(x, y) \) is a joint probability distribution / probability mass function  of discrete RV \( X, Y \) if:</p>
<ol>
<li>\( f(x, y) \geq 0 \)</li>
<li>\( \sum_x \sum_y f(x, y) = 1 \)</li>
<li>\( P(X = x, Y = y)= f(x, y) \)</li>
</ol>
<p>\( \forall \) region \( A \) in the \( xy \) plane, \( P[(X,Y) \in A] = \sum \sum_A f(x, y) \)</p>
<blockquote>
<p>e.x if, for a car that needs service, \( X \) denotes distance driven on a set of tires and \( Y \) denotes no. of tires to be replaced, then \( f(10000, 2) \) is the probability that it has driven for over 10000 distance units and the car needs 2 new tires.</p>
</blockquote>
<hr>
<p>For example, if we select 2 ballpoint pens from a box containing 3 blue/2 red/3 green pens, and if \( X \) is the no. of blue pens selected and \( Y \) the no. of red pens selected,</p>
<p>The joint probability function is the set is</p>
<p>$$f(x,y) = (0,0), (0,1), (1,0), (1,1), (0,2), (2,0)$$</p>
<p>And for \( A = \{ (x, y) | x + y \leq 1 \}\)</p>
<p>$$ P[(X, Y) \in A] = P( X + Y \leq 1 ) = f(0,0) + f(0,1) + f(1, 0)  \\
= \frac{3}{28} + \frac{3}{14} + \frac{9}{28} = \frac{9}{14} $$</p>
<p>The same can be applied to continuous probability distributions as well</p>
<hr>
<p><em>Define</em>: \( f(x, y) \) is a joint density function of continuous RV \( X, Y \) if:</p>
<ol>
<li>\( f(x, y) \geq 0 \)</li>
<li>\( \sum_{-\infty}^{\infty} \sum_{-\infty}^{\infty} f(x, y) = 1 \)</li>
<li>\( P[(X, Y) \in A] = \int \int_A f(x, y) dx dy\) for any region A in the \( xy \) plane</li>
</ol>
<blockquote>
<p>TLDR; integrating over whole region gives probability of 1, integrating over a subregion gives the probability of an event within that range.</p>
</blockquote>
<hr>
<p>But what if we want to inspect the probabilities dist. of a specific RV within the joint distribution?</p>
<hr>
<p><em>Define</em>: the marginal distributions of \( X \), \( g(x) \), given joint distribution \( f(x, y) \), is:</p>
<p>for discrete case:</p>
<p>$$ g(x) = \sum_y f(x,y) $$</p>
<p>for continuous case:</p>
<p>$$ g(x) = \int_{-\infty}^{\infty} f(x, y) dy$$</p>
<blockquote>
<p>The same follows for finding the marginal distribution of \( Y \), \( h(y) \)</p>
</blockquote>
<hr>
<hr>
<p><em>Define</em>: Conditional probability distribution \( P(y|x) \)</p>
<p>$$ f(y|x) = \frac{f(x,y)}{g(x)}, g(x) &gt; 0$$</p>
<hr>
<p>For example if we want to find if a discrete random variable \( X \) falls in between \( a  \) and \( b \) when \( Y \) is known, we can evaluate</p>
<p>$$ P(a &lt; X &lt; b | Y = y) = \int_a^bf(x|y)dx = \int_a^b \frac{f(x,y)}{h(y)}dx $$</p>
<hr>
<p><em>Define</em>: Statistical Independence</p>
<p>Given RV \( X \) and \( Y \) with joint probability distribution \( f(x, y) \) and marginal distributions \( g(x) \) and \( h(y) \) respectively, \( X \) and \( Y \) are statistically independent <em>if and only if</em></p>
<p>$$ f(x,y) = g(x)h(y) $$</p>
<p>for all values of \( x, y \) in their range.</p>
<p>We can generalize this to a set of random variables \( X, Y, Z, &hellip; \) with joint probability distribution \( f(x, y, z, &hellip;) \) and marginal distributions \( g(x), h(y), i(z), &hellip; \) respectively.</p>
<p>\( X, Y, Z, &hellip; \) are statistically independent <em>if and only if</em></p>
<p>$$ f(x, y, z, &hellip;) = f_x(x) f_y (y)f_z(z)&hellip;$$</p>
<blockquote>
<p>Recall: doesn&rsquo;t this look a lot like when we did all the linear independence stuff back in MAT185?</p>
</blockquote>
<hr>
<hr>
<p><em>Define</em>: Expectation/Mean of RV</p>
<blockquote>
<p>Or: What would the average value be in the long run if we keep on sampling from the distribution</p>
</blockquote>
<p>Let \( X \) be a random variable with probability distribution \( f(x) \)</p>
<p>$$ \mu = E[X] = \int_{-\infty}^\infty x f(x) dx $$</p>
<p>if \( g(X) \) is a function of \( X \),</p>
<p>$$ \mu = E[g(X)] = \int_{-\infty}^\infty g(x)f(x) dx $$</p>
<p>and this can be generalized to joint distributions as well</p>
<p>$$ \mu = E[g(X, Y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x, y)f(x, y) dx dy $$</p>
<p>This value is of special importance as it describes where the probability distribution is centered.</p>
<hr>
<hr>
<p><em>Define</em>: Variance</p>
<blockquote>
<p>Or: how much the distribution spreads out</p>
</blockquote>
<p>For finding the variance of a random variable \( g(X) \):</p>
<p>Discrete:
$$ \sigma^2 = E[(g(X)-\mu)^2] = \sum_x (x-\mu)^2 f(x)$$</p>
<p>Continuous:
$$ \sigma^2 = E[(g(X)-\mu)^2] = \int_{-\infty}^{\infty} (x-\mu)^2 f(x)dx   $$</p>
<blockquote>
<p>to find the variance of just \( X \), use \( g(X) = X \)</p>
</blockquote>
<p>A simplified form of the above to find variance of RV \( X \):</p>
<p>$$ \sigma^2 = E(X^2) - \mu^2 $$</p>
<hr>
<hr>
<p><em>Define</em>: Covariance</p>
<p>Given RV \( X, Y \) and  joint probability distribution \( f(x, y) \), how closely are they associated?</p>
<p>Discrete:
$$ \sigma_{XY} = E[(X-\mu_X)(Y-\mu_Y)] = \sum_x \sum_y f(x,y) (x-\mu_X)(y-\mu_Y) $$</p>
<p>Continuous:</p>
<p>$$ \sigma_{XY} = E[(X-\mu_X)(Y-\mu_Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x,y) (x-\mu_X)(y-\mu_Y) dx dy $$</p>
<p>A simplified form of the above:</p>
<p>$$ \sigma_{XY} = E(XY) - \mu_x\mu_y $$</p>
<ul>
<li>If large values of \( X \) often result in large values of \( Y \), the covariance is positive.</li>
<li>If large values of \( X \) often result in small values of \( Y \), the covariance is negative.
<ul>
<li>Sign of covariance denotes positive/negative relationship between \( X \), \( Y \).</li>
<li>If \( X, Y \) are statistically independent, the covariance is 0.</li>
</ul>
<blockquote>
<p>Note: the reverse is not always true since covariance only describes a linear relationship between two RV</p>
</blockquote>
</li>
</ul>
<p>Correlation coefficient of \( X, Y \):</p>
<p>$$ \rho_{XY} = \frac{\sigma_{XY}}{\sigma_X\sigma_Y} $$</p>
<hr>
<h4 id="linear-combinations-of-random-variables">Linear Combinations of Random Variables</h4>
<p>\( p(x) \) is linear if it satisfies the linearity test:</p>
<ul>
<li>\( p(ax+b) = ap(x) + bp(x) \) for constant \( a \), constant \( b \).</li>
</ul>
<p>With the help of some integrals we can show that expectation is linear as well:</p>
<p>$$ E[aX + b] = aE[X] + b $$</p>
<p>or more generally,</p>
<p>$$ E[g(X) \pm h(X)] = E[g(X)] \pm E[h(X)] $$</p>
<p>The same also applies to functions of &gt;1 random variables.
This leads to a number of properties, the one of note that isn&rsquo;t immediately obvious is that</p>
<p>$$ E(XY) = E(X)E(Y)$$</p>
<blockquote>
<p>the above theorem may be applied to show that if X, Y are two independent random variables, then \( \sigma_{XY} = 0 \)</p>
</blockquote>
<hr>
<p><em>Theorem</em>: if \( X \) and \( Y \) are random variables with joint probability distribution \( f(x, y) \) and \( b, c \) are constants, then</p>
<p>$$ \sigma_{aX + bY + c} = a^2\sigma_X^2 + b^2\sigma_Y^2 + 2ab\sigma_{XY} $$</p>
<blockquote>
<p>If \( X \) and \( Y \) are random variables, what would the covariance of a linear combination of them be?
Note that the covariance doesn&rsquo;t depend on \( c \)</p>
</blockquote>
<hr>
<h4 id="discrete-distributions">Discrete Distributions</h4>
<hr>
<p><em>Define</em>: Binomial Distribution</p>
<p>A discrete distribution that models the number of successes in a Bernoulli process, which is a process that satisfies the following:</p>
<ol>
<li>An experiment that may be repeated many times</li>
<li>Each trial has a boolean outcome</li>
<li>The outcome probability is trial-constant</li>
<li>Repeated trials are independent</li>
</ol>
<p>Some properties:</p>
<p>Probability of \(  x \) 1s and \( 1-x \) 0s in some order is \( p^x(1-p)^{n-x} \)</p>
<p>No. of ways to have \(  x \) 1s and \( 1-x \) 0s is
$$ \binom{n}{x} $$</p>
<p>Therefore the binomial distribution may be presented as:</p>
<p>$$ b(x; n, p) =  \binom{n}{x} p^x(1-p)^{n-x} $$</p>
<p>Expectation:
$$ E[X] = \sum_{x=0}^n x \binom{n}{x} p^x(1-p)^{n-x} = np $$</p>
<p>Variance:</p>
<blockquote>
<p>Note: each trial is independent \( \sigma_{Y_j Y_k} = 0 \qquad, k \neq j \)</p>
</blockquote>
<p>$$ \sigma_X^2 = np(1-p) $$</p>
<hr>
<hr>
<p><em>Define</em>: Multinomial Distribution</p>
<blockquote>
<p>A discrete distribution that can have \( m \) outcomes instead of the  \( 2 \) of the binomial distribution.</p>
</blockquote>
<p>$$ f(x_1, x_2 &hellip;, x_k; p_1, p_2, &hellip; p_k, n) \\
= \binom{n}{ x_1 &hellip; x_k } p_1^{x_1}, p_2^{x_2} &hellip; p_k^{x_k}  $$</p>
<hr>
<hr>
<p><em>Define</em>: Hypergeometric Distribution</p>
<blockquote>
<p>Hypergeometric distributions model sampling without replacement, whereas multinomial distributions model sampling with replacement</p>
</blockquote>
<p>A hypergeometric distribution \( h(x; N, n, k) \) is the probability distribution of the hypergeometric random variable which represents the no. of successes of a hypergeometric experiment (that&rsquo;s a lot of hypergeometric), which is an experiment where:</p>
<ol>
<li>A random sample of size \( n \) is drawn \( N \) items without replacement</li>
<li>Of the \( N \) items, \( k \) are successes and \( N-k \) are failures.</li>
</ol>
<p>Mean:
$$ \mu = \frac{nk}{N} $$
Variance:
$$ \sigma^2 = \frac{N-n}{N-1} \cdot n \cdot \frac{k}{N} ( 1 - \frac{k}{N} ) $$</p>
<p>PMF:</p>
<p>$$ h(x; N, n, K) = \frac{\binom{K}{x}\binom{N-K}{n-x}}{\binom{N}{n}} $$</p>
<hr>
<h4 id="negative-binomial--geometric-distributions">Negative Binomial &amp; Geometric Distributions</h4>
<blockquote>
<p>Instead of finding the probability of \( x \) successes in a fixed \( n \) trials, negative experiments are interested in the probability that the \( k \)th success occurs on the \( x \)th trial</p>
</blockquote>
<hr>
<p><em>Define</em>: Negative Binomial</p>
<p>$$ b*(x; k, p) = \binom{x-1}{k-1} p^k(1-p)^{x-k} $$</p>
<p>Intuition:</p>
<ul>
<li>on the \( x \)th trial the chance of \( k-1 \) successes in the last \( x-1 \) trials is \( b(k-1; x-1, p) = \binom{x-1}{k-1}p^{k-1}(1-p)^{x-k} \)</li>
<li>Extend that to \( x, k \), observe that \( b*(x;k,p) = pb(k-1;x-1, p) \) and so forth</li>
</ul>
<hr>
<hr>
<p><em>Define</em>: Geometric Distribution</p>
<blockquote>
<p>Repeated trials with probability of success \( p \); can be thought of as a negative binomial distribution with \( k = 1 \)</p>
</blockquote>
<p>$$ g(x;p) = b*(x; 1, p) = p(1-p)^{x-1} $$</p>
<p>Properties</p>
<ul>
<li>\(  \mu = \frac{1}{p} \)</li>
<li>\(  \sigma^2 = \frac{(1-p)}{p^2} \)</li>
</ul>
<blockquote>
<p>Example application: playing a game where the opponent wins 90% of the time. How many games until the first win? \( g(x; 0.1) = 0.1(0.9)^{x-1} = 1/0.1 = 10 \)</p>
</blockquote>
<hr>
<h4 id="poisson-distribution">Poisson Distribution</h4>
<p>A Poisson distribution models the probability distribution of the Poisson random variable which represents the numerical output of Poisson experiments in a Poisson process</p>
<blockquote>
<p>That&rsquo;s a lot of Poisson!</p>
</blockquote>
<p>A Poisson process has the following properties:</p>
<ol>
<li>The number of outcomes occurring during one interval is independent of others, i.e. the process is stateless</li>
<li>The probability that a single outcome will occur in an interval is proportional to the size of the interval and does not depend on the no. of outcomes outside of that interval</li>
<li>The probability that &gt; 1 outcome will occur in such a short time interval or fall in a small one is negligible</li>
</ol>
<hr>
<p><em>Define</em>: Poisson probability distribution</p>
<p>$$ p(x, \lambda t) = \frac{e^{-\lambda t} ( \lambda t )^x}{x!} \qquad, x \ge  0 \in \mathbb{Z}$$</p>
<ul>
<li>\( \lambda \) is the rate of occurrence of the event per unit in interval</li>
<li>Mean: \( \mu = \lambda \)</li>
<li>Variance: \( \sigma^2 = \lambda \)</li>
</ul>
<hr>
<p>Note that we can also write the Poisson distribution as a limit of the binomial distribution</p>
<p>$$ \lim_{n \to \infty, p \to 0} b(x; n, p) \\
= \lim_{n \to \infty, p \to 0} \frac{n(n-1) &hellip; (n-x+1)}{x!} \lambda/n)^x(\frac{\lambda}{\lambda/n})^{n-x} \\
= \lim_{n \to \infty, p \to 0} \frac{n^x}{x!} (\lambda/n)^x (1-\lambda/x)^{n-x} \\
= \frac{\lambda^x}{x!} \lim_{n \to \infty, p \to 0} e^{-\lambda}  = \frac{\lambda^x}{x!}e ^ {-\lambda}  \\
= p(x; \lambda)  \\ $$</p>
<h4 id="uniform-distribution">Uniform Distribution</h4>
<hr>
<p><em>Define</em>: Uniform Distribution</p>
<p>\( X \) has PDf</p>
<p>$$ f(x; A, B) = \begin{cases}
\frac{1}{B-A} &amp; \text{if } x \in [A, B] \\
0 &amp; \text{if } x \notin [A, B]
\end{cases} $$</p>
<p>Mean:</p>
<p>$$ \mu = E[X] = \int_{-\infty}^{\infty} x f(x; A, B) dx = \int_A^B \frac{x}{B-A} dx = \dots = \frac{B+A}{2} $$</p>
<p>Varience:
$$ \sigma^2 = E[(x-a)^2] = \frac{(B-A)^2}{12} $$</p>
<hr>
<p>Often used in describing nature. Rolling dies (discrete uniform distribution). Is also really easy to do calculations with.</p>
<h4 id="normal-gaussian-distribution">Normal (Gaussian) Distribution</h4>
<blockquote>
<p>It&rsquo;s somewhat special so we denote it by \( u \)</p>
</blockquote>
<p>$$ u(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi} } \exp(-\frac{(x-\mu)^2}{2\sigma^2}) \qquad x \in \mathbb{R} $$</p>
<p>


 <img loading="lazy" src="img/normal_dist.png" alt="normal_dist"  />

</p>
<blockquote>
<p>\( \sigma \) will tell us how &ldquo;flat&rdquo; or &ldquo;peaked&rdquo; it is; the spread, whereas \( \mu \) tells us its shift to the left/right</p>
</blockquote>
<p>This seems like an awfully unintuitive distribution. Let&rsquo;s break it down and build intuition.</p>
<h5 id="is-it-a-pdf">Is it a PDF?</h5>
<p>If we integrate it over the whole domain, do we get
$$ \int_{-\infty}^{\infty} n(x; \mi, \sigma) dx = 1 \qquad ?$$</p>
<p>This isn&rsquo;t really a straightforwards integral to solve &ndash; there&rsquo;s a trick to it: apply polar coordnates.</p>
<ol>
<li>Try multiplying them together.</li>
</ol>
<blockquote>
<p>1 x 1 = 1</p>
</blockquote>
<p>$$ \int_{-\infty}^{\infty} n(x; \mu, \sigma) dx \cdot \int_{-\infty}^{\infty} n(y; \mu, \sigma) dy \\
=\int_{-\infty}^{\infty}  \int_{-\infty}^{\infty}  n(x, \mu, \sigma)n(y; \mu, \sigma) dx dy$$</p>
<blockquote>
<p>Recall: \( r^2 = x^2 + y^2\) and \( dxdy = r dr d\theta \)</p>
</blockquote>
<p>$$ = \frac{1}{2\pi\sigma^2} \int_0^{2\pi} \int_0^\infty \exp{\frac{-r^2}{2\sigma^2}} r dr d\theta \\
= \frac{2\pi}{2\pi\sigma^2} \int_0^\infty \exp{\frac{-r^2}{2\theta^2}} r dr$$</p>
<p>Apply u-substitution to the integral, \( s = r^2, ds = 2rdr\)</p>
<p>$$ = - \frac{2\sigma^2}{2\sigma^2} \exp{1\frac{s}{2\sigma^2}} |^\infty_0 \\
= -(0-1) = 1$$</p>
<p>And therefore it is a PDF.</p>
<h5 id="what-is-the-mean">What is the mean?</h5>
<p>Apply the definition of expectation
$$ E[X] = \int_{-\infty}^{\infty} x  \frac{1}{\sigma \sqrt{2\pi} } \exp(-\frac{(x-\mu)^2}{2\sigma^2}) \qquad x \in \mathbb{R}  dx $$</p>
<p>Apply a u-sub to integral, \( z = \frac{x-\mu}{\sigma} \)</p>
<p>$$ \frac{\sigma}{\sqrt{2\pi}} \int_{-\infty}^{\infty}   z \exp{(-z^2/2)} dz + \frac{\mu}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{(-z^2/2)} dz  = \mu$$</p>
<p>The first term is an odd function so it will cancel out to \( 0 \).
The 2nd term is a normal PDF, i.e. \( n(x; 0, 1) \) which will integrate to \( 1 \), and as such this whole thing evaluates to \( \mu \)</p>
<h5 id="variance">Variance?</h5>
<p>$$ E[(X-\mu)^2] = \sigma^2 $$</p>
<p>


 <img loading="lazy" src="img/normal_dist.png" alt=""  />

</p>
<blockquote>
<p>Notice the relation between \( \sigma \) and the spread of the PDF.</p>
</blockquote>
<blockquote>
<p>Recall, <em>PDF</em> is the probability density function which when integrated over the whole domain, gives 1. The <em>CDF</em> is the <em>running integral</em> of the PDF, i.e. is increasing, of range \( (0, 1)\), and approaches 1 as \( X \rightarrow \infty \). From this result it is fairly intuitive that \( P(A \leq X \leq B) \) = \( P(B) - P(A) \)</p>
</blockquote>
<h5 id="standard-normal-distribution">Standard normal distribution</h5>
<p>A special form of the normal distribution is the standard normal distribution</p>
<p>$$ n(x; 0, 1) = \frac{1}{\sqrt{2\pi}} \exp{x^2/2} $$</p>
<p>The CDF of the standard normal distribution is denoted as \( \Phi(X) \)</p>
<p>$$ \Phi(X) = \int_{-\infty}^{x} n(t; 0, 1) dt $$</p>
<p>And it follows that for a standard normal RV \( X \),</p>
<p>$$ P(A \leq X \leq B) = \Phi(B) - \Phi(A) $$</p>
<p><strong>But why do we need a standard normal distribution?</strong></p>
<p>Consider any \( n(x; \mu, \sigma) \) distribution &ndash; there is no analytical solution to the integral \( \int_{-\infty}^x n(t; 0, 1) dt \) <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>So we can only computationally get the CDF of the normal distribution, which is not ideal.</p>
<blockquote>
<p>But can we compute \( n(x; \mu, \sigma) \) using \( \Phi(X) \)?</p>
</blockquote>
<p>As it turns out &ndash; yes we can. Let</p>
<p>$$ Z = \frac{x-\mu}{\sigma} \qquad \text{where} X \text{has} n(x; \mu, \sigma) $$</p>
<p>Consider</p>
<p>$$ \begin{aligned}
P(X \leq x) &amp;= \int_{-\infty}^{x} n(t; \mu, \sigma) dt \cr
&amp;= \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}\sigma} \exp{-\frac{(t-\mu)^2}{2\sigma^2}} dt \cr
(\quad s &amp;= \frac{t-\mu}{\sigma}, dt = \sigma ds \qquad \dots 	\text{apply u-substitution: }) \cr
&amp;=  \int_{-\infty}^{\frac{x-\mu}{\sigma}} \frac{1}{\sqrt{2\pi} \sigma } \exp{-s^2/2} \quad \text{note how this looks like standard normal dist.}\cr
&amp;= \Phi(\frac{x-\mu}{\sigma}) \cr
&amp;= P(Z = (x-\mu)/\sigma) \cr
\end{aligned} $$</p>
<hr>
<p><strong>Example</strong>: Let \( X \) be a normal RV with PDF \( n(x; 5, 2) \). Find \( P(-1 \leq X \leq 4 ) \) w.r.t. the CDF of the standard normal distribution</p>
<p>By inspection,
$$ 	\text{Let} Z = \frac{X-5}{2} $$</p>
<p>then,</p>
<p>$$ P(-1 \leq X \leq 4) = P( \frac{-1-5}{2} \leq \frac{x-5}{2} \leq \frac{4-5}{2} ) \\ = P(-3 \leq Z \leq 0.5) = \Phi(-0.5) - \Phi(-3) \approx 0.3072$$</p>
<hr>
<p>The normal distribution can be used to approximate the binomial distribution.</p>
<ul>
<li>\( n trials \</li>
<li>i.e. for a coin flip &amp; modelling # of heads: \( b(x; n, p) = \binom{n}{x} p^x (1-p)^{n-x} \)</li>
<li>mean: \( np \), variance: \( np(1-p) \)</li>
</ul>
<p>$$ Z = \frac{x-np}{\sqrt{np(1-p)} }$$</p>
<ul>
<li>note that for many things as \( n \rightarrow \infty \) the distribution of \( Z \) appears to be a standard normal distribution.</li>
</ul>
<h4 id="gamma-distribution">Gamma Distribution</h4>
<hr>
<p><em>Define</em>: Gamma function</p>
<p>$$ \Gamma(\alpha) = \int_{0}^{\infty} x^{\alpha - 1} e^{-x} dx   $$</p>
<hr>
<p>The gamma distribution is a distribution that makes use of the gamma function.
it is not very intuitive.</p>
<p>$$ f(x; \alpha, \beta) = \begin{cases}
\frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha - 1} e^{-\frac{x}{\beta}} &amp; \text{for } x \geq 0 \\
0 &amp; \text{otherwise}
\end{cases} $$</p>
<p>$$ \alpha &gt; 0, \beta &gt; 0 $$</p>
<ul>
<li>\( E[ X  ] = \mu = \alpha\beta \)</li>
<li>\( \sigma^2 = \alpha\beta^2 \)</li>
</ul>
<p>However a lot of <em>much nicer</em> distributions can be derived from the gamma distribution.</p>
<hr>
<p><em>Define</em>: Chi-Squared (\( \chi^2 \)) distribution</p>
<p>$$ f(x; v) = \frac{1}{2^{\frac{v}{2}}\Gamma(\frac{v}{2})} x^{\frac{v}{2} -1} e^{-\frac{x}{2}} \qquad x &gt; 0 $$</p>
<blockquote>
<p>This is the gamma distribution with \( \alpha = 2 \).</p>
</blockquote>
<p>Good for describing confidence intervals, and for describing the distribution of a sample of a population.
See: <a href="https://en.wikipedia.org/wiki/Chi-squared_test">chi-squared test</a></p>
<hr>
<hr>
<p><em>Define</em>: Exponential distribution</p>
<p>$$ f(x; \beta) = \begin{cases}
\frac{1}{\beta} e^{-\frac{x}{\beta}} &amp; \text{for } x &gt; 0 \\
0 &amp; \text{otherwise}
\end{cases} $$</p>
<blockquote>
<p>this is the gamma distribution with \( \alpha = 1 \).</p>
</blockquote>
<p>Properties:</p>
<ul>
<li>\( E<input checked="" disabled="" type="checkbox"> = \beta \)</li>
<li>\( \sigma^2 = \beta^2 \)</li>
<li>Relates to the Poisson distribution</li>
<li>Is memory-less</li>
</ul>
<hr>
<h5 id="exponential-distribution--poisson-distribution">Exponential Distribution &amp; Poisson Distribution</h5>
<blockquote>
<p>Recall: for a Poisson distribution, \( p(x; \lambda) = \frac{e^{-\lambda} x^\lambda}{x!} \), \( \mu = \lambda \)
We can interpret this as \( \lambda = rt \).</p>
</blockquote>
<p>If buses arriving at a bus stop was a Poisson process, then the time interval between the bus arrivals would be an exponential distribution.</p>
<p>For example, if we&rsquo;re trying to find the probability of <em>no arrivals</em> within \( x \),</p>
<p>$$ p(0, rx) = e^{-rx} $$</p>
<p>Let \( X \) be a RV for the time of the first arrival.</p>
<blockquote>
<p>The probability of no arrivals in that time frame is the same as the probability of the first arrival to be after time \( x \).</p>
</blockquote>
<p>Then, given</p>
<p>$$ P(X \geq x) = p(0; rx) = e^{-rx}  $$</p>
<p>We have</p>
<p>$$ P(X &lt; x) = p(0; rx) = 1 - e^{-rx}  $$</p>
<blockquote>
<p>This is the CDF of the RV \( X \)</p>
</blockquote>
<p>Apply fundamental theorem of calculus</p>
<p>$$  F(X) = 1-e^{-rx} $$</p>
<p>then we get the PDF</p>
<p>$$ f(x) = \frac{d}{dx} F(x) = re^{-rx} $$</p>
<p>Which is an exponential distribution with \( r = 1/\beta \).</p>
<h5 id="exponential-distribution--memory-less-ness">Exponential Distribution &amp; Memory-less-ness</h5>
<p>Given RV \( X \) with \( f(x, \beta) = \frac{1}{\beta} e^{-x/\beta} \), consider</p>
<p>$$ P(X \geq s + t) \qquad X \geq s $$</p>
<p>Can split this up into two parts and then evaluate&hellip;</p>
<p>$$ \begin{aligned}
&amp;=  \frac{P(X \geq x+t \cap X \geq s)}{P(X \geq s)} \cr
&amp;=  \frac{P(X \geq x+t)}{P(X \geq s)} \cr
&amp;= \frac{
\int_{s+t}^{\infty} \frac{1}{\beta} e^{\frac{-x}{\beta}} dx
}{
\int_{s}^{\infty} \frac{1}{\beta} e^{\frac{-x}{\beta}} dx
} \cr
&amp;= \frac{
e^{-\frac{s+t}{\beta}}
}{
e^{-\frac{s}{\beta}}
} \cr
&amp;= e^{-\frac{-t}{\beta}}  \cr
&amp;= P(X \geq t) \cr
\end{aligned} $$</p>
<blockquote>
<p>i.e. what&rsquo;s the probability of \( s+t \) if we&rsquo;ve already waited up to \( s \)? I.e. if we&rsquo;ve already waited \( s \) seconds, then the following probability  (\( s + t \)) is entirely unaffected by those initial \( s \) seconds.</p>
</blockquote>
<h2 id="ece259-electromagnetism">ECE259: Electromagnetism</h2>
<p>Electromagnetic field is a vector quantity given by a magnitude \( E \) and unit vector \( \hat{a} \), and is commonly found as the force exerted on a positive test charge \( q \).</p>
<p>$$ \vec{E}= E\hat{a}_E = \frac{\hat{F}}{q} = q\vec{E}$$</p>
<p>We can expand on this to find the force exerted by a point charge at varying distances \( r \) from the charge, noting that \( k = \frac{1}{4\pi E_o}\), where \( E_o \) is the permittivity of free space.</p>
<p>$$ \vec{E} = \frac{1}{k} \frac{q_{source}}{r^2} \hat{a}_e $$</p>
<p>and this can be further generalized for a system of charges that are not necessarily at the origin</p>
<p>$$ \vec{E_{sys}} = \frac{1}{k}\sum_k \frac{q_k}{|\vec{R}-\vec{R'_k}|^3}(\vec{R} - \vec{R'_k}) $$</p>
<h3 id="vector-calculus-review">Vector Calculus Review</h3>
<h4 id="coordinate-systems">Coordinate Systems</h4>
<p><strong>Cylindrical \(( r_p, \phi_p, z_p)\)</strong>:</p>
<p><strong>Note</strong>: When adding vectors in non-Cartesian coordinate systems we cannot just add the components because the <em>direction</em> of the unit vectors can change.</p>
<blockquote>
<p>When adding vectors in non-Cartesian coordinate systems we must first convert the vectors to Cartesian coordinates</p>
</blockquote>
<p><strong>Cylindrical -&gt; Cartesian</strong>:
$$ r_p = \sqrt{x_p^2 + y_p^2}  $$
$$ \phi_p = \arctan(y_p/x_p) $$
$$ z_p = z_p $$</p>
<p><strong>Cartesian -&gt; Cylindrical</strong>:
$$ x_p = r_p \cos(\phi_p) $$
$$ y_p = r_p \sin(\phi_p) $$
$$ z_p = z_p $$</p>
<p><strong>Spherical \(( r_p, \theta_p, \phi_p)\)</strong>:</p>
<blockquote>
<p>\( \theta \) gives angle w.r.t the z-axis, \( \phi \) gives angle w.r.t the x-axis</p>
</blockquote>
<p><strong>Cartesian -&gt; Spherical</strong>:
$$ r_p = \sqrt{x_p^2 + y_p^2 + x_p^2}  $$
$$ \theta_p = \arccos{\frac{z}{\sqrt{x_p^2 + y_p^2 + z_p^2}  }}$$
$$ \phi_p = \arctan(y_p/x_p) $$</p>
<p><strong>Spherical -&gt; Cartesian</strong>:
$$ x_p = r_p \sin{\theta_p}\cos{\phi_p} $$
$$ y_p = r_p \sin{\theta_p}\sin{\phi_p} $$
$$ z_p = r_p \cos{\theta_p}$$</p>
<p>


 <img loading="lazy" src="img/coordinate_transformations.png" alt=""  />

</p>
<blockquote>
<p>From reference sheet</p>
</blockquote>
<h4 id="integration">Integration</h4>
<p>Multidimensional integration in different coordinate systems can largely be done just by finding the appropriate differential element and then inserting it into the appropriate formula.</p>
<ul>
<li>Line integrals:
$$ -\int_A^B{\vec{E} \cdot d \vec{l}} $$</li>
</ul>
<blockquote>
<p>example application: finding potential difference</p>
</blockquote>
<ul>
<li>Surface integrals:</li>
</ul>
<p>$$ \int_S{\vec{D}\cdot d\vec{s}} $$</p>
<blockquote>
<p>Example application: finding flux, etc.</p>
</blockquote>
<ul>
<li>Volume integrals:</li>
</ul>
<p>$$\int_V{ P dV } $$</p>
<blockquote>
<p>example application: finding total charge</p>
</blockquote>
<ul>
<li>
<p>Cartesian
$$ d\vec{l} = d_x\vec{a_x} + d_y\vec{a_y} + d_z \vec{a_z}  $$
$$ d\vec{s} = d_y d_z \vec{a_x} + d_x d_z \vec{a_y} + d_x d_y \vec{a_z} $$
$$ dV = d_xd_yd_z $$</p>
</li>
<li>
<p>Cylindrical</p>
</li>
</ul>
<p>$$ d\vec{l} = d_r\vec{a_r} + d_\phi \vec{a_\phi} + d_z\vec{a_z}$$
$$ d\vec{s} = r d_\phi d_z \vec{a_r} + d_r d_z \vec{a_\phi} + r d_\phi d_r \vec{a_z} $$
$$ dV = r d_\phi d_zd_r $$</p>
<ul>
<li>Spherical</li>
</ul>
<p>$$ d\vec{l}  = d_R \vec{a_r} + Rd\theta \vec{a_\theta} + R\sin{\theta}d_\phi \vec{a_\phi} $$
$$ d\vec{s} = R^2\sin{\theta}d_\theta d\phi \vec{a_r} + R\sin{\theta}dRd\phi\vec{d_\phi} $$
$$ dV = R^2\sin{\theta}d_Rd_\theta d_\phi $$</p>
<p>A general approach for integration is as follows:</p>
<ol>
<li>Choose a coordinate system</li>
<li>Determine which position coordinates vary during integrationa nd their ranges</li>
<li>Select the appropriate differential quantity, e.g. \( d\vec{l} \), \( d\vec{s} \), \( dV \), etc.</li>
<li>If the integrand is a vector, make sure that all unit vectors are constant during integration</li>
<li>Integrate over the appropriate limit</li>
</ol>
<p>It is often useful to convert between types of integrals to make the process easier.
This can be done with the following theorems:



 <img loading="lazy" src="img/vector_integral_properties.png" alt="vector_integral_properties"  />

</p>
<blockquote>
<p><a href="http://furqaanyusaf.com/notes/vectors/stokes-apply">source</a></p>
</blockquote>
<h3 id="electric-fields">Electric Fields</h3>
<h4 id="continuous-charge-distribution">Continuous charge distribution</h4>
<blockquote>
<p>Recall: \( \vec{E_{sys}} = \frac{1}{4\pi\varepsilon_o}\sum_k \frac{q_k}{|\vec{R}-\vec{R'_k}|^3}(\vec{R} - \vec{R'_k}) \)</p>
</blockquote>
<p>In continuous form, \( \int{d\vec{E}} \)</p>
<p>Steps for solving charge distribution problem:</p>
<ol>
<li>Choose coordinate system</li>
<li>Write out  \( dQ \)</li>
<li>Write out \( \vec{R} - \vec{R'} \)</li>
<li>Write \( d\vec{E} \)</li>
<li>Integrate!</li>
</ol>
<h5 id="drawing">Drawing</h5>
<p>


 <img loading="lazy" src="img/e_field_draw.png" alt="e_field_draw"  />

</p>
<ul>
<li>\( \vec{E} \) must be perpendicular to conductors</li>
<li>Equipotential lines are perpendicular to \( \vec{E} \) lines</li>
</ul>
<h5 id="volume-charge">Volume Charge</h5>
<p>$$ dQ = P_v dv' $$</p>
<blockquote>
<p>Contribution to \( \vec{E} \). \( P \) denotes volume charge density [C/m^3]</p>
</blockquote>
<p>$$ \vec{E} = \frac{1}{4\pi\varepsilon_o}  \int_{V'} \frac{P_v}{|\vec{R}-\vec{R'}|^2} \hat{a}_{\vec{R} - \vec{R'}} dV' $$</p>
<p>Noting that</p>
<p>$$ \hat{a}_{\vec{R} - \vec{R}} = \frac{\vec{R} - \vec{R'}}{|\vec{R} - \vec{R'}|}$$</p>
<p>we obtain that
$$ \vec{E} = \frac{1}{4\pi\varepsilon_o}  \int_{V'} \frac{P_v}{|\vec{R}-\vec{R'}|^3} (\vec{R} - \vec{R'}) dV' $$</p>
<p>Much the same approach can be taken for surface and line charges; they all end up having the same form but with \( ds' \) or \( dl' \)), respectively.</p>
<h4 id="flux--gausss-law">Flux &amp; Gauss&rsquo;s Law</h4>
<ul>
<li>Electric fields extend (with decreasing density) from point charges (away from charge for +&rsquo;ve, towards charge for -&rsquo;ve).
<ul>
<li>A good way to &ldquo;visualize&rdquo; this is to draw field lines.</li>
</ul>
</li>
</ul>
<p>


 <img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Camposcargas.svg/440px-Camposcargas.svg.png" alt="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Camposcargas.svg/440px-Camposcargas.svg.png"  />

</p>
<p>Note that greater field line density corresponds to greater field strength. We usually aren&rsquo;t concerned with field strength at a point, however, but rather it&rsquo;s effect on surfaces.</p>
<hr>
<p><em>Define</em>: Flux: the electric field through a surface</p>
<p>$$ \Phi = \int_s \vec{E} \cdot d\vec{s} $$</p>
<p>where \( d\vec{s} \) denotes a normal vector to the surface.</p>
<blockquote>
<p>Note the dot product! This also means that a surface parallel to the field lines will have a flux of 0.</p>
</blockquote>
<hr>
<hr>
<p><em>Define</em>: Gauss&rsquo;s law (Integral form)</p>
<p>$$ \Phi_E = \oint_s \vec{E}\cdot d\vec{s} = \frac{Q}{\varepsilon_o} $$</p>
<ul>
<li>Take integral over a closed surface (gaussian surface)</li>
<li>\( Q \) denotes charge <em>inside</em> surface and is equal to \( \int P_v dv' \) or \( P_v \) multiplied by the volume in question</li>
</ul>
<blockquote>
<p>Total flux <em>out</em> of a surface is equal to the (total charge enclosed by surface)/(permittivity of free space)<br>
if \( \oint \vec{E} \cdot d\vec{s} \) &gt; 0; net flux out (+&rsquo;ve charge enclosed), &lt; 0; net flux in (-&rsquo;ve charge enclosed)</p>
</blockquote>
<p>It can also be written in differential form using the divergence theorem:</p>
<blockquote>
<p>Recall: \( \int_v \vec{\nabla} \cdot \vec{A} dv = \oint_s \vec{A} \cdot d\vec{s} \))</p>
</blockquote>
<p>$$ \vec{\nabla} \cdot A = \lim_{\Delta V \to 0} \frac{\oint_S \vec{A} \cdot d \vec{s}}{\Delta V} $$</p>
<p>where the value in the limit denotes the net outward flux of A per unit volume</p>
<blockquote>
<p>Recall: \( \vec{\nabla} = \frac{d}{dx}\vec{a_x} +  \frac{d}{dy}\vec{a_y} + \frac{d}{dz}\vec{a_z} + \)</p>
</blockquote>
<p>we may then apply the divergence theorem to gauss&rsquo;s law to obtain</p>
<p>$$ \oint_s \vec{E}\cdot d\vec{s} = \frac{\int_v Pv dv}{\varepsilon_o}
, \therefore \vec{\nabla \cdot E} = \frac{P_v}{\varepsilon_o} $$</p>
<hr>
<p>


 <img loading="lazy" src="img/flux_ex.png" alt=""  />

</p>
<p>


 <img loading="lazy" src="img/image_2022-01-28-20-05-05.png" alt="image_2022-01-28-20-05-05"  />

</p>
<blockquote>
<p>The electric field reaches a maximum at \( R = b \), then is the maximum value multiplied by a factor of \( \frac{1}{R^2} \) after \( R &gt; b \). Like how a point charge behaves,  which is what we expect.</p>
</blockquote>
<h4 id="gausss-law-differential-form">Gauss&rsquo;s law: differential form</h4>
<p>Recall:</p>
<ul>
<li>\( \vec{\nabla} V = grad V \) &lt;- gradient</li>
<li>\( \vec{\nabla} \cdot \vec{A} = div A \)  &lt;- divergence</li>
<li>\( \vec{\nabla} \times \vec{A} = curl A \)  &lt;- curl</li>
</ul>
<blockquote>
<p>Previously we used the integral form to solve Gauss&rsquo;s law &ndash; but the differential form is equally valid:</p>
</blockquote>
<p>


 <img loading="lazy" src="img/e_field_ex_diff_form.png" alt=""  />

</p>
<h4 id="work--energy">Work &amp; Energy</h4>
<p>We know that the electrostatic force experienced by a charge is \( \vec{F} = q\vec{E}\).
Combining this with the definition of work, we can write the work done by an external force moving a point charge \( q \) from point \( A \to B \) as:</p>
<p>$$ W_{A\to B} = \int_A^B \vec{F_{ext}} \cdot d\vec{l} = -q\int_A^B \vec{E} \cdot d\vec{l}$$</p>
<p>Also, electric fields are <a href="https://en.wikipedia.org/wiki/Electric_field">conservative</a>, therefore</p>
<p>$$ W_{B\to A} = -W_{A \to B} $$</p>
<p>and it follows that</p>
<blockquote>
<p>Apply Stoke&rsquo;s theorem</p>
</blockquote>
<p>$$ \oint_l \vec{E} \cdot d\vec{l} = \vec{\nabla} \times \vec{E} 0$$</p>
<p>which leads to two fundamental electrostatic properties&hellip;</p>
<h4 id="postulates-of-electrostatics-in-free-space">Postulates of Electrostatics (in free space)</h4>
<ol>
<li>\( \vec{\nabla} \cdot \vec{E} = \frac{P_v}{\varepsilon_o} \)</li>
<li>\( \vec{\nabla} \times \vec{E} = 0 \)</li>
</ol>
<p>This also enables us to define electric potential</p>
<p>$$ \vec{\nabla} \times (\vec{\nabla } V) = 0 \\
since \qquad \vec{\nabla } \times \vec{E} = 0, \\
define \qquad \vec{E} = -\vec{\nabla } V  $$</p>
<p>where \( V \) denotes potential.</p>
<h4 id="electric-potential">Electric Potential</h4>
<p>$$ \Delta V_{A \to B} = V_B - V_A = \frac{\Delta U_{A \to B}}{q} = - \int_A^B \vec{E}\cdot d \vec{l}$$</p>
<ul>
<li>\(  q \) is a test charge</li>
</ul>
<blockquote>
<p>Potential is relative and is associated with the field (it is independent of the test charge)</p>
</blockquote>
<p>We may define potential a little more rigorously to inspect its differential form:</p>
<blockquote>
<p>Recall: the gradient points in the direction of maximum slope</p>
</blockquote>
<p>$$
\begin{aligned}
\Delta V_{A \to B} &amp;= -\int_A^B \vec{E} \cdot d\vec{l} \cr
&amp;= \int_A^B \frac{dV}{dn} \frac{dn}{dl} dl \cr
&amp;= \int_A^B \int_A^B \frac{dV}{dn} \vec{a_n} \vec{a_l} dl  \cr
&amp;= \int_A^B \vec{\nabla}V \cdot d\vec{l} \cr
\therefore \quad \vec{E} &amp;= -\vec{\nabla}V
\end{aligned}$$</p>
<p>Applying this to a number of different charges we may derive the following:</p>
<ol>
<li>Potential due to point charge</li>
</ol>
<p>$$ \begin{aligned}
V &amp;= -\int_\infty^R \vec{E} \cdot d\vec{l}\cr
&amp;= -\int_\infty^R \frac{q_1}{4\pi \varepsilon_0 R^2} \vec{a_r} \cdot \vec{a_r} d\vec{R}\cr
&amp;= -\int_\infty^R \frac{q_1}{4\pi \varepsilon_0 R^2} dR \cr
\vdots \cr
V(R) &amp;= \frac{q_1}{4\pi \varepsilon_0 R}
\end{aligned} $$</p>
<blockquote>
<p>V(R) denotes the potential at a point of interest \( P \) that is distance \( R \) from the charge</p>
</blockquote>
<ol start="2">
<li>Potential due to discrete charge distribution (i.e. many point charges)</li>
</ol>
<blockquote>
<p>Electric field is conservative, so we can just sum up a modified version of the single point charge form found in 1.</p>
</blockquote>
<p>$$ V(R) = \sum_k \frac{q_k}{4\pi\varepsilon_o |\vec{R} - \vec{R'_k}| } $$</p>
<blockquote>
<p>Specify \( R \) as the distance from the point of interest and the charge source, \( | \vec{R} - \vec{R'_k} | \)</p>
</blockquote>
<ol start="3">
<li>Electric dipole: a pair of equal and opposite charges separated by a small distance \( d \  \)</li>
</ol>
<ul>
<li>Dipole moment: \( \vec{P} = q\vec{d} \)
<ul>
<li>In spherical coordinates: \( \vec{p} = p\cos\theta \vec{a_r} - p\sin\theta\vec{a_\theta} \)</li>
</ul>
</li>
</ul>
<p>Potential of a dipole can be written as the sum of two closely spaced charges:</p>
<p>$$ V = \frac{1}{4\pi\varepsilon_o} (\frac{-q}{| \vec{R} + \frac{\vec{d}}{2} |} + \frac{q}{| \vec{R} - \frac{\vec{d}}{2} |} ) $$</p>
<blockquote>
<p>Note directions of the charges and \( d \). Also we&rsquo;re only going to consider \( R \gg d \)</p>
</blockquote>
<p>Begin by simplifying the expressions for the distances</p>
<p>$$ \begin{aligned}
\frac{1}{|\vec{R} - \frac{\vec{d}}{2}|}&amp;= \frac{1}{\sqrt{(\vec{R} - \vec{d}/{2})\cdot(\vec{R} - \vec{d}/{2})}}   \cr
&amp;= \frac{1}{\sqrt{\vec{R}\cdot \vec{R} - \vec{R} \cdot \vec{d} + \frac{\vec{d} \cdot \vec{d}}{4}} } \cr
\vdots \cr
&amp;=  \frac{1}{R}(1+\frac{\vec{R} \cdot \vec{d}}{2R^2}) \cr
\end{aligned} $$</p>
<p>And much the same can be done for the other term.
Plugging into the expression for the potential we get:</p>
<p>$$ V = \frac{1}{4\pi\varepsilon_o R^2} p \cos \theta $$</p>
<blockquote>
<p>V is a function of \( R \) and \( \theta \)</p>
</blockquote>
<p>Next we find the field associated with this dipole.
This is made easy via \( \vec{E} = -\vec{\nabla} V \)</p>
<p>$$ \begin{aligned}
\vec{E} &amp;= -(  \frac{\delta V}{\delta R} \vec{a_R}  + \frac{1}{R} \frac{\delta V}{\delta \theta} \vec{a_\theta}   \frac{1}{R\sin\theta}\frac{\delta V}{\delta \phi} \vec{a_\phi}   ) \cr
&amp;= \frac{2P\cos\theta}{4\pi\varepsilon R^3} \vec{a_R} + \frac{P\sin\theta}{4\pi\varepsilon R^3} \vec{a_\theta} \cr
\end{aligned} $$</p>
<blockquote>
<p>Notice now \( \vec{e} \propto 1/R^3 \)</p>
</blockquote>
<h4 id="electric-potential-for-continuous-charges">Electric Potential for continuous charges</h4>
<p>$$ V = \frac{1}{4\pi \varepsilon_o} \int_x \frac{\rho_x}{R} dx  $$</p>
<p>where \( x \) can be either a line, surface, or a volume (and the integral would be dimensioned accordingly) depending on the charge distribution.</p>
<p>For example, finding \( V, \vec{E}\) for a point perpendicular to a flat disk charge of radius \( b \) with a uniform charge density of \( \rho_s \):</p>
<p>


 <img loading="lazy" src="img/uniform_charge_dist_pot_ex.png" alt="uniform_charge_dist_pot_ex"  />

</p>
<h4 id="conductors">Conductors</h4>
<ul>
<li>\( \sigma \) denotes conductivity, \( \frac{1}{\sigma} \) denotes resistivity</li>
<li>Good conductors have loosely coupled electrons and partially filled CB, \( \sigma \approx 6 \times 10^7 \) (copper)</li>
<li>Semiconductors have somewhat tightly bound electrons, \( \sigma \approx 1.5 \times 10^{-3 }\) (silicon)</li>
<li>Insulators have very confined electrons, conductivity near zero e.g. \( \sigma \approx 10^{-14}\) (rubber)</li>
<li>A perfect conductor has \( \sigma \longrightarrow \infty \), a perfect insulator has \( \sigma \longrightarrow 0 \)</li>
<li>Electric fields are zero in an perfect conductor [<a href="https://physics.stackexchange.com/questions/147739/rigorously-prove-that-electric-field-is-zero-in-a-perfect-conductor">proof</a>]</li>
</ul>
<ol>
<li>uncharged conductor in \( \vec{E} \) field</li>
</ol>
<p>


 <img loading="lazy" src="img/e_field_conductor.png" alt="e_field_conductor"  />


Intuition: electric field induces charges to move around in the conductor. This induces an &ldquo;internal electric field&rdquo; (denoted in green) and it turns out that in equilibrium this will perfectly cancel out the external field. For proof consider if this did not cancel out &ndash; this would imply violation of conservation laws</p>
<ol start="2">
<li>charged conductor in \( \vec{E} \) field</li>
</ol>
<p>


 <img loading="lazy" src="img/e_field_conductor_w_charge.png" alt="e_field_conductor_w_charge"  />

</p>
<blockquote>
<p>If the conductor has a charge, i.e. some electrons scattered about</p>
</blockquote>
<p>In an \( \vec{E} \) field they will redistribute in order for the internal field to fully cancel out the external one.</p>
<h5 id="perfect-conductors-in--vece--field-summary">Perfect conductors in \( \vec{E} \) field: summary</h5>
<ol>
<li>\( \vec{E} \) = 0</li>
<li>\( P_v \) = 0</li>
<li>\( V = \text{constant} \); there is uniform potential everywhere</li>
</ol>
<p>$$ \oint_{cycle} = \int_{\text{in conductor}} + \int_{\text{at surface}} = 0 $$



 <img loading="lazy" src="img/e_field_int_cond.png" alt="e_field_int_cond"  />

</p>
<p>Since electric fields are conservative the cycle integral is zero.
The internal field \( \vec{E} = 0\), therefore \( \int_{\text{in conductor}} = 0 \).
So how can we have \( \int_{\text{at surface}} = 0 \) when there is a non-zero electric field?
This is because we are looking at \( \int_{\text{at surface}} \), which is <em>tangential</em> to the surface (whereas the field lines radiate normal to the surface and so are perpendicular to what we&rsquo;re integrating over), which explains why it evaluates to \( 0 \).</p>
<ul>
<li>But what about the normal component?</li>
</ul>
<p>


 <img loading="lazy" src="img/e_field_int_normal.png" alt="e_field_int_normal"  />

</p>
<p>We can identify a Gaussian surface and apply gauss&rsquo;s law</p>
<p>$$ \oint_s \vec{E} \cdot d \vec{s} = \frac{Q}{\varepsilon} = \int_{\text{top}} \vec{E} \cdot d \vec{s} + \int_{\text{bottom}} \vec{E} \cdot d \vec{s} + \int_{\text{sides}} \vec{E} \cdot d \vec{s} $$</p>
<p>Applying symmetry we get \( \int_{\text{sides}}  = 0 \), and \( \int_{\text{bottom}} = 0 \) since \( \vec{E} = 0 \) in the conductor&hellip;we can evaluate this using Gauss&rsquo;s law:</p>
<p>$$ \therefore \qquad \vec{E_n} = \frac{Q}{\Delta S \varepsilon_o} = \frac{P_s}{\varepsilon_o} $$</p>
<p>So the potential in a conductor is constant and equal to the potential in its outer surface.</p>
<p>


 <img loading="lazy" src="img/image_2022-02-08-13-59-02.png" alt="image_2022-02-08-13-59-02"  />

</p>
<blockquote>
<p>There is an error here somewhere since potential should go higher and higher as \( R \) increases&hellip; &ldquo;Exercise left for the reader&rdquo;</p>
</blockquote>
<h5 id="perfect-dielectric-conductors-in--vece-">Perfect Dielectric conductors in \( \vec{E} \)</h5>
<ul>
<li>Dielectrics have no moving charges ( no free electrons ), however there can be bond charges; dielectrics can be polarized (and hence contribute to electric potential)</li>
<li>Also, some materials are made of molecules with non-zero dipole moments, e.g. \( H_2O \); external \( \vec{E} \) fields can make them align with the field</li>
<li>Some materials, <a href="https://en.wikipedia.org/wiki/Electret">electrets</a> can can exhibit permanent electric dipole moment even without an external field and can be induced by <a href="https://en.wikipedia.org/wiki/Glass_poling">thermal poling</a></li>
</ul>
<p>Polarization vector \( \vec{P} \)</p>
<p>$$ \vec{P} = \lim_{\Delta_v \to 0} \frac{\sum_{k=1}^{n\delta v \vec{P_k}}}{\delta v} \qquad [\frac{C}{m^2}] $$</p>
<ul>
<li>\( n \) = number of particles per unit volume</li>
<li>\( \vec{P_k} \) = dipole moment of microscopic particles \( C-m \)</li>
<li>\( \vec{P} \) = volume density of electric dipole moment; is a point function</li>
</ul>
<p>


 <img loading="lazy" src="img/polarization_calc_diagram.png" alt="polarization_calc_diagram"  />

</p>
<p>How does potential relate to polarization?</p>
<p>$$ d\vec{p} = \vec{P} d\vec{v} \xrightarrow[\text{electric potential}]{\text{produces}} dV = \frac{\vec{p} \cdot\vec{a_r}}{4\pi \varepsilon_o R^2} dv' $$</p>
<ul>
<li>\( dV \) is the contribution to potential from dipole moment in \( dv' \)</li>
</ul>
<p>$$ V(\vec{R}) = \int_{v'} dV = \int_{v'} \frac{\vec{P} \cdot \vec{a}_{\vec{R} - \vec{R'}}}{4 \pi \varepsilon_o |\vec{R} - \vec{R}' |^2} dv' $$</p>
<p>And then a bunch of messy math&hellip;</p>
<p>$$ \begin{aligned}
V(\vec{R}) &amp;=  \cr
&amp; \frac{1}{4\pi\varepsilon_o} \oint_{S\prime} \frac{\vec{P} \cdot \vec{a\prime}_{n}}{|\vec{R} - \vec{R}\prime |} ds\prime \cr
&amp; +  \frac{1}{4\pi\varepsilon_o} \int_v \frac{-\vec{\nabla \prime} \cdot \vec{P}}{|\vec{R} - \vec{R}\prime | dv\prime}  \cr
\end{aligned} $$</p>
<ul>
<li>The first term is the potential produced by a surface charge distribution on \( S\prime \) with a density of\( \rho_{ps} = \vec{p} \cdot \vec{a^\prime_n} \)</li>
<li>The second term is the potential produced by a volume charge distribution in \( v\prime \) with a density of \( \rho_{pv} = -\vec{\nabla} \cdot \vec{p} \)</li>
</ul>
<blockquote>
<p>Katex isn&rsquo;t allowing me to do a \( \int_{v'} \) in the above so just pretend that the \( \int_v \) is \( \int_{v'} \)</p>
</blockquote>
<p>The physical interpretation is that a dielectric material may have its dipoles aligned after being in a \( \vec{E} \) field and thus be able to produce an \( \vec{E} \) field itself.</p>
<p>$$  $$</p>
<h4 id="generalized-gausss-law">Generalized Gauss&rsquo;s Law</h4>
<p>In free space:</p>
<p>$$ \vec{\nabla }\cdot \vec{E} = \frac{\rho_p}{\varepsilon_o}  $$
$$ \oint_s \vec{E} \cdot d\vec{s} = \frac{Q_end}{\varepsilon_o} $$</p>
<p>In a medium:</p>
<ul>
<li>Must take the polarization charge \( \rho_{pv} \) as well as the free</li>
</ul>
<p>$$ \vec{\nabla }\cdot \vec{E} = \frac{\rho_p + \rho_{pv} }{\varepsilon_o} \\
\Rightarrow \vec{\nabla } \cdot(\varepsilon\vec{E}) = P_v + ( \vec{\nabla } \cdot \vec{p}  ) \\
\Rightarrow \vec{\nabla } \cdot (\varepsilon \vec{E} + \vec{P}) = \rho_v $$
Define displacement vector \( \vec{D} \) as:</p>
<p>$$ \vec{D} = \varepsilon \vec{E} + \vec{p} $$</p>
<blockquote>
<p>Units of static flux density \( [C/m^2] \)</p>
</blockquote>
<p>This brings us to the generalized Gauss&rsquo;s law:</p>
<hr>
<p><em>Define</em>: Generalized Gauss&rsquo;s Law</p>
<p>$$ \vec{\nabla} \cdot \vec{D} = \rho $$</p>
<p>where \( \rho \) is the <strong>free</strong> charge density. In integral form,</p>
<p>$$ \oint_S \vec{D} \cdot d\vec{s} = Q_{end} $$</p>
<hr>
<p>Now we may write out the postulates of electrostatics in more general form, i.e. accounting for non-free space as well</p>
<ul>
<li>\( \vec{\nabla} \cdot \vec{D} = \rho \Leftrightarrow \oint_s \vec{D} \cdot d \vec{s} = Q \)</li>
<li>\( \vec{\nabla} \times \vec{E} = 0 \Leftrightarrow \oint_l \vec{E} \cdot d\vec{l} = 0\)</li>
</ul>
<p><strong>For a linear and isotropic material</strong>&hellip;</p>
<blockquote>
<p>isotropic means that \( \vec{p} and \vec{E} \) is in the same direction, i.e. if we apply a field to the material it will polarize \( p \) in the same direction as the electric field.</p>
</blockquote>
<p>$$ \vec{p} = \varepsilon_o \chi_e \vec{E} $$</p>
<ul>
<li>\( \chi \) denotes electrical susceptibility which is unitless. \( \chi = 0 \) in a vacuum.</li>
</ul>
<p>$$ \vec{D} = \varepsilon_o \vec{E} + \vec{p} \xrightarrow{\text{linear, isotropic}} \varepsilon_o \vec{E}(1 + \chi_e) = \varepsilon_o \varepsilon_r \vec{E} = \varepsilon \vec{E}  $$</p>
<ul>
<li>\( \varepsilon \)  denotes the absolute permittivity</li>
<li>\( \varepsilon_r \)  denotes the relative permittivity, aka dielectric constant</li>
</ul>
<p>For example, a positive point charge \( Q \) at the centre of a dielectric shell</p>
<p>


 <img loading="lazy" src="img/polaization_charge_ex.png" alt="polaization_charge_ex"  />

</p>
<h2 id="bme205-introduction-to-biomedical-engineering">BME205: Introduction to Biomedical Engineering</h2>
<blockquote>
<p>Textbook: Human Physiology From Cells to Systems, 5th Canadian Edition by Sherwood and Ward (ISBN-13 978-0-17-691235-2).</p>
</blockquote>
<h3 id="cells">Cells</h3>
<blockquote>
<p>Reference: Chapter 2</p>
</blockquote>
<h4 id="components">Components</h4>
<ul>
<li>
<p><strong>Plasma Membrane</strong>: lipid bilayer studded with proteins; acts as a selective barrier between inside of cell and outside world</p>
</li>
<li>
<p><strong>Nucleus</strong>: &ldquo;Control center&rdquo; of cell; genetic material (DNA in Chromosomes) storage</p>
<ul>
<li>Chromatin</li>
<li>Nucleolus</li>
<li>Nuclear pores</li>
<li>Cisternae</li>
</ul>
</li>
<li>
<p><strong>Endoplasmic Reticulum</strong>: Membranous network of fluid-filled tubules and flattened sacs studded with ribosomes; forms new cell components and materials for secretion</p>
<ul>
<li>Rough Endoplasmic Reticulum</li>
<li>Smooth Endoplasmic Reticulum</li>
</ul>
</li>
<li>
<p><strong>Lysosome</strong>: Sacs containing hydrolytic enzymes; &ldquo;digestive system&rdquo; of cell</p>
</li>
<li>
<p><strong>Peroxisome</strong>: Sacs containing oxidative enzymes; detoxification system</p>
</li>
<li>
<p><strong>Centriole</strong>: Barrel-shaped organelles composed of 9 short triplet microtubules; site of microtubule growth</p>
</li>
<li>
<p><strong>Microtubule</strong>: slender and hollow tubes made of secretory vesicles; forms the mitotic spindle and aid in cellular transport</p>
</li>
<li>
<p><strong>Mitochondria</strong>: &ldquo;Powerhouse of the cell&rdquo;: ATP production</p>
</li>
<li>
<p><strong>Ribosome</strong>: Workbenches for protein synthesis</p>
</li>
<li>
<p><strong>Vesicle</strong>: Transient membranous sacs for product transport</p>
</li>
<li>
<p><strong>Microfilaments</strong>: Intertwined chains of actin molecules (in muscles, myosin molecules) &ndash; for cellular contractile systems</p>
</li>
<li>
<p><strong>Intermediate Filaments</strong>: Irregular threadlike proteins to resist mechanical stress</p>
</li>
<li>
<p><strong>Cytoplasm</strong>: Jelly-like &ldquo;fluid&rdquo; inside of cells</p>
</li>
</ul>
<h4 id="cell-membrane">Cell membrane</h4>
<ul>
<li>Cell membranes (plasma membranes) are formed of a phospholipid bilayer (hydrophilic phosphate+glycerol&quot;head&quot;, hydrophobic fatty acid &ldquo;tail&rdquo;).</li>
</ul>
<p>


 <img loading="lazy" src="img/plasma_membrane.png" alt="plasma_membrane"  />

</p>
<ul>
<li>Phospholipid molecules can move around the PM via
<ul>
<li>Uncatalyzed transverse diffusion (slow, days) (think of pairs in bilayers flipping)</li>
<li>Flippase-catalyzed diffusion (fast, seconds)</li>
<li>Lateral diffusion (very fast, micrometer/second) (think of of a molecule moving around the bilayer)</li>
</ul>
</li>
</ul>
<h5 id="biological-glue">Biological Glue</h5>
<ul>
<li><strong>Desmosome</strong>: a tight &ldquo;spot weld&rdquo; between cells</li>
<li><strong>Tight Junction</strong>: multiple &ldquo;seams&rdquo; between cells; forms strong semi-impermeable (selective) bond between cells. Often found in intestinal tissue</li>
<li><strong>Gap Junction</strong>: Cells coupled by &ldquo;connexon&quot;s a structure that allows for passage of ions and small molecules. Can be opened/closed to vary passage. Leaves a 2-4nm gap between cells.</li>
</ul>
<p>


 <img loading="lazy" src="img/junctions1.png" alt="junctions1"  />





 <img loading="lazy" src="img/junctions2.png" alt="junctions2"  />

</p>
<h5 id="osmosis">Osmosis</h5>
<ul>
<li>
<p><strong>Osmosis</strong>: movement of solvent from areas of high solvent concentration -&gt; areas of low solvent concentration</p>
<ul>
<li>Typically only lets in small molecules and ions</li>
<li>Larger ones must be transported across the bilayer using carrier proteins</li>
</ul>
</li>
<li>
<p>Cell uses active transport when it must transport molecules/ions <em>against</em> the concentration gradient</p>
</li>
</ul>
<p>


 <img loading="lazy" src="img/active_transport.png" alt="active_transport"  />

</p>
<blockquote>
<p>Illustration of \( Na^+/K^+ \) pump</p>
</blockquote>
<h5 id="ion-leak-channels">Ion Leak Channels</h5>
<p>&ldquo;Ion leak channels&rdquo; are passive highly selective channels that allow for passage of ions in and out of the cell.
Some can be gated and some are not.</p>
<p>


 <img loading="lazy" src="img/image_2022-01-27-16-20-27.png" alt="image_2022-01-27-16-20-27"  />

</p>
<blockquote>
<p>Key concentrations and relative permeability of ion leak channels</p>
</blockquote>
<hr>
<p><em>Define</em>: Nernst Equation</p>
<p>$$ E_x = \frac{61}{Z_x}\log_10\frac{[C]_o}{[C]_i}$$</p>
<ul>
<li>E = equliibrium potential (mV)</li>
<li>z = valence of permeant ion (e.g. Na+ = +1, Ca+ = +2, Cl- = -1)</li>
<li>[C]o = concentration of ion outside cell (mol/L)</li>
<li>[C]i = concentration of ion inside cell (mol/L)</li>
</ul>
<p>There is also a full form of the equation:</p>
<blockquote>
<p>Idea: there is an osmosis concentration gradient across the membrane as well as a charge gradient</p>
</blockquote>
<p><em>Example</em>: Sodium ions across a cell membrane, outside concentration of 150, inside concentration of 15</p>
<p>$$ E_{Na^+} = \frac{61}{+1}\log \frac{150}{15} = +61mV$$</p>
<ul>
<li>Note: \( E_{ion} \neq  V_m \)</li>
</ul>
<hr>
<h4 id="atp">ATP</h4>
<p>ATP (<strong>A</strong>denosine <strong>T</strong>ri<strong>P</strong>hosphate) enables cellular activity through the energy released from breaking the high-energy phosphate bonds it contains.</p>
<p>$$\text{ATP} \rightarrow \text{ADP} + \text{P} + \text{energy}$$</p>
<blockquote>
<p><em>Anabolic</em> activities forms complex molecules from simple ones; <em>Catabolic</em> activities decompose complex molecules into simple ones.</p>
</blockquote>
<p>ATP involves three different pathways:</p>
<ul>
<li>substrate-level phosphorylation</li>
<li>anaerobic glycolysis</li>
<li>aerobic metabolism
Most cells generate ATP through the 4-step process glycolysis process:</li>
</ul>
<ol>
<li>glycolysis (aerobic + anaerobic)</li>
</ol>
<ul>
<li>10 sequential steps that break glucose down into pyruvic acod molecules</li>
<li>not very efficient; yields only 2x ATP and 2x NADH per molecule of glucose</li>
</ul>
<ol start="2">
<li>decarboxylation of pyruvate</li>
<li>tricarboxylic acid cycle (TCA cycle, aerobic)</li>
<li>electron transport chain (aerobic)</li>
</ol>
<p>However substrate-level phosphorylation is often used to immediately generate ATP, i.e. by muscle cells during intense exercise.
This is done by using <strong>creatine phosphate</strong> (CP) as a substrate and catalyzing the ATP synthesis process with <strong>creatine kinase</strong>.
This reaction is reversible, i.e. ATP can be produced via CP and vice-versa.</p>
<p>$$ CP + ADP \xrightarrow{\text{creatine kinase}} \text{creatine} + ATP$$</p>
<h4 id="dna--chromosomes">DNA &amp; Chromosomes</h4>
<p>DNA is built up of nucleotides, each of which has three components: a nitrogenous base, a five-carbon sugar <em>deoxyribose</em>, and a phosphate group.
These are linked together by hydrogen bonds between bases, which is highly specific: <em>adenine (A)</em>: <em>thymine (T)</em>,  <em>guanine (G)</em>: <em>cytosine (C)</em>.
A and G have double ring structures while C and T have single ring structures.</p>
<p>Chromosomes are made of DNA that has been &ldquo;super coiled&rdquo; (think: telephone cord). Humans have 23 pairs of chromosomes.</p>
<p>


 <img loading="lazy" src="img/dna_chromosome.jpg" alt="dna_chromosome"  />

.</p>
<p>There are multiple ways that this can be packed, e.g. zigzag or solenoid patterns:</p>
<p>


 <img loading="lazy" src="img/nucleosome_packing.jpg" alt="nucleosome_packing"  />





 <img loading="lazy" src="img/coiling_patterns.jpg" alt="coiling_patterns"  />

</p>
<h5 id="replication">Replication</h5>
<blockquote>
<p>Note: DNA is formed of two complimentary strands <a href="https://en.wikipedia.org/wiki/Directionality_(molecular_biology)">(5', 3' strands)</a>.</p>
</blockquote>
<ol>
<li>DNA is first split apart by proteins called <strong>initiators</strong> which look for <em>origin</em> base sequences
<ul>
<li>There are many origins along the DNA strand since it would take too long to split the DNA from just one point.</li>
</ul>
</li>
<li>With the opening made, <em>helicase</em> enzymes gets to business to really start breaking the DNA apart.
<ul>
<li>Think of a zipper being unzipped in two ways.</li>
<li><em>Single-stranded binders</em> keep the DNA propped open and <em>gyrase</em> prevents it from torquing and knotting.
Then, the actual replication step takes place</li>
</ul>
</li>
<li>A piece of temporary RNA bonds with the first nucleotide opened by helicase</li>
<li><em>DNA polymerase</em> comes along and pairs up bases</li>
<li>After the bases are all put in, the temporary RNA is broken off and the DNA fragments are joined together by <em>ligase</em></li>
<li><em>proofreading DNA polymerase</em> goes along the DNA and checks for errors by looking for &ldquo;bumps&rdquo; in the structure
And then it is put into the chromosomal form</li>
<li>DNA is wrapped around histones &amp; <em>telomeres</em> (single-sided pieces of &ldquo;junk DNA&rdquo; at the end) managed by trimming them to equal length in mitosis or being wrapped with RNA primer and <em>telomerase</em> enzyme in meiosis.</li>
</ol>
<h5 id="gene-expression">Gene Expression</h5>
<blockquote>
<p>Why don&rsquo;t skin sprout eyeballs or livers grow toes?</p>
</blockquote>
<p>Gene expression is the mechanism by which only the appropriate genes are called upon to produce the desired function.
Usually the default setting for gene expression is to be off.</p>
<p>Things that can impact gene expression:</p>
<ul>
<li>Enviromental factors (usu. heat/light)</li>
<li>Genetic presets (e.g. some that are supposed to activate at certain stages of development; cues taken from cell or neighbouring cells)</li>
<li>Hormones; chemicals produced by brain and glands</li>
</ul>
<p>When a gene gets switched on it gets &ldquo;transcribed&rdquo; into a piece of <em>messenger RNA</em>, which is a single stranded nucleic acid which carries the information out of the nucleus and into the cell.</p>
<blockquote>
<p>Note: RNA contains Uracil (U) instead of Thymine (T) as a compliment to Adenine (A).</p>
</blockquote>
<p>A key property of RNA is its additional oxygenation, which makes it a lot more reactive.
This, combined with the fact that U can bond with all the other bases gives the ability for RNA to form complex non-linear shapes.</p>
<p>Another property of RNA is that it is is single-use; each piece of mRNA ceases functioning after it&rsquo;s task has been completed.</p>
<h5 id="transcription">Transcription</h5>
<h4 id="nerve-cells">Nerve Cells</h4>
<blockquote>
<p>Recall: there are ion channels in the cell membrane. T</p>
</blockquote>
<h2 id="phy294-quantum-and-thermal-physics">PHY294: Quantum and Thermal Physics</h2>
<h3 id="schrodinger--the-hydrogen-atom">Schrodinger &amp; the Hydrogen Atom</h3>
<hr>
<p><em>Define</em>: Schrodinger&rsquo;s Equation</p>
<p>$$ H\psi = E\psi $$</p>
<ul>
<li>\( H \) is the Hamiltonian, \( \psi \) is the wave function, and \( E \) is the energy.</li>
<li>\( |\psi|^2 \) gives the probability density function.</li>
</ul>
<p>Generalizing to three dimensions:</p>
<p>$$ \frac{\delta^2}{\delta x^2} + \frac{\delta^2\psi}{\delta y^2} + \frac{\delta^2\psi}{\delta z^2} = \frac{2M}{\hbar^2}[U-E] \psi  \\
\frac{1}{r}\frac{\delta^2 \psi}{\delta r^2}(r\psi) + \frac{1}{r^2sin\theta} \frac{\delta}{\delta \theta} (\sin \theta \frac{\delta \psi}{\delta\theta}) + \frac{1}{r^2 \sin^2 \theta} \frac{\delta^2 \psi}{\delta \phi^2} = \frac{2M}{\hbar^2}[U-E]\psi$$</p>
<ul>
<li>\( U \) denotes potential (in H atom these are columbic forces)</li>
<li>\( E \) is energy non-dependent on distance to the nucleus</li>
<li>\( M \) is mass</li>
</ul>
<hr>
<blockquote>
<p>Recall: for a 1D particle in a box we use \( \psi = Asin(kx) + Bcos(kx)\) and then we can apply the boundary conditions at the bounds of the box.
We may then find \( \psi \) to be \( \sqrt{\frac{2}{L}}sin()\frac{n\pi}{L})x\) and \( E_n = \frac{n^2h^2}{8mL^2} \)) where \( n \)) is a integer &gt; 0.</p>
</blockquote>
<p>In 2D and 3D potential wells this is more complicated but the same idea follows; we apply separation of variables and the boundary conditions.</p>
<h4 id="2d-potential-well">2D Potential Well</h4>
<ol>
<li>Write out 2D Schrodinger&rsquo;s equation</li>
</ol>
<p>$$ \frac{\delta^2\psi}{\delta x^2} + \frac{\delta^2\psi}{\delta y^2} = \frac{2M}{\hbar^2}[U-E]\psi  $$</p>
<p>For an infinite potential well,</p>
<p>$$ U(x,y) = \begin{cases}
0 &amp; 0 \leq x \leq a \text{ and } 0 \leq y \leq a \cr
\infty &amp; \text{otherwise} \cr
\end{cases} $$</p>
<ol start="2">
<li>Identify boundary conditions</li>
</ol>
<ul>
<li>Since \( U = 0 \) in the box, \( E \) can take on all non-zero values.</li>
<li>The particle cannot escape from the box; \( \psi (x, y) = 0 \) @ the boundaries
Therefore we can reduce the Schrodinger equation to</li>
</ul>
<p>$$ \frac{\delta^2\psi}{\delta x^2} + \frac{\delta^2\psi}{\delta y^2} = \frac{2ME}{\hbar^2}\psi  $$</p>
<ol start="3">
<li>Use separation of variables</li>
</ol>
<p>$$ \psi(x, y) = X(x) Y(y) $$</p>
<p>$$ \frac{\delta^2\psi}{\delta x^2} + \frac{\delta^2\psi}{\delta y^2} = \frac{2ME}{\hbar^2}(X(x) Y(y))$$</p>
<blockquote>
<p>Equations like the above reduced Schrodinger equation guarantee that <em>any</em> solution can be expressed as a sum of separated solutions \( \rightarrow \) once we find all solutions of form \( X(x) Y(y) \) we will have found all possible solutions.</p>
</blockquote>
<p>&hellip; and then do a bunch of math.</p>
<p>$$ \begin{aligned}
\frac{\partial^2 \psi}{\partial x^2} &amp;= Y(y) \frac{\partial^2  }{\partial x^2} X(x)   \cr
\frac{\partial^2 }{\partial x^2} X(x)  &amp;= \frac{d^2}{dx^2}X(x) = X''(x) \cr
\vdots \cr
&amp; \therefore \frac{\partial^2 \psi}{\partial x^2} = Y(y)X''(x) \cr
&amp; \therefore \frac{\partial^2 \psi}{\partial y^2} = X(x)Y''(y) \cr
\end{aligned} $$</p>
<blockquote>
<p>Can be simplified by taking \( y \) to be fixed while looking for \( \frac{\partial^2 \psi}{\partial x^2} \) and vice-versa</p>
</blockquote>
<p>Next substitute them into the Schrodinger equation</p>
<p>$$ Y(y) X''(x) + X(x) Y''(y) = -\frac{2ME}{\hbar^2} X(x) Y(y) $$</p>
<p>divide through with \( Y(y) X(x) \) to get something of form \( f(x) + g(y) = C \)</p>
<p>$$ \frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)} = -\frac{2ME}{\hbar^2} $$</p>
<p>This tells us that \( X(x) \), which can only depend on \( x \), does not depend on \( x \) and therefore implies that \( X''(x)/X(x) \) is a constant.
Taking this constant to be \( -k_x^2 \) we obtain</p>
<p>$$ X''(x) = -k_x^2X(x) $$</p>
<p>which is of the same form as the Schrodinger&rsquo;s equation for a 1D potential well.</p>
<p>The same argument can be made for \( Y \).</p>
<ol start="4">
<li>
<p>Apply boundary conditions</p>
</li>
<li>
<p>\( X(x) = 0 \qquad x = 0 , a \)</p>
</li>
<li>
<p>\( Y(y) = 0 \qquad y = 0 , a \)</p>
</li>
</ol>
<p>We know the solution for a 1D potential well (\( X(x) = B\sin{k_x x}\)); \( k_x = \frac{n_x\pi}{a}, \qquad n &gt; 1 \in \mathbb{Z} \)</p>
<p>Therefore we obtain</p>
<p>$$ X(x) = B \sin{\frac{n_x \pi x}{a}} \\
Y(y) = C \sin{\frac{n_y \pi y}{a}} $$</p>
<p>And since \( \psi(x,y) = X(x) Y(y) \)</p>
<p>$$ \psi(x,y) = BC\sin(k_x x) \sin{(k_y y)} = A \sin{\frac{n_x \pi x}{a}} \sin{\frac{n_y \pi y}{a}} =$$</p>
<ol start="5">
<li>Identify allowed energies</li>
</ol>
<p>Recall:
$$ Y(y) X''(x) + X(x) Y''(y) = -\frac{2ME}{\hbar^2} X(x) Y(y) $$</p>
<p>Substitute what we have derived since (i.e. \( X''/X = -k_x^2 \)) and we get
$$ E = \frac{\hbar^2 \pi^2}{2Ma^2}n^2 \qquad [n = 1,2,3\dots] $$</p>
<blockquote>
<p>Note quantized energy</p>
</blockquote>
<h4 id="central-force-problem">Central Force Problem</h4>
<p>When applying to a hydrogen a few changes have to be made since it is not linear/square/cubic/etc and is spherical instead.
This problem is called the &ldquo;<a href="https://en.wikipedia.org/wiki/Hydrogen-like_atom">central force problem</a>&rdquo; and is solved in much the same way as the square/rectangular potential wells, except using spherical coordinates.
In two dimensions we use much the same method as prior with the 2D infinite potential well, but in polar coordinates.</p>
<p>$$ \frac{\delta^2\psi}{\delta x^2} + \frac{\delta^2\psi}{\delta y^2} = \frac{1}{r}\frac{\delta^2 \psi}{\delta r^2} + \frac{1}{r}\frac{\delta \psi}{\delta r} + \frac{1}{r^2}\frac{\delta \psi^2}{\delta \phi} $$</p>
<p>And then apply separation of variables \( \psi(r, \phi) = R(r) \Phi(\phi) \).
This has much the same procedure as before so will skip over it to find that we get all solutions as \( \Phi(\phi) = e^{im\phi} \).
Applying Euler&rsquo;s identity we note that \( \psi(r, \phi) = \psi(r, \phi + 2\pi) \) which implies that the wave function (and the separated \( \Phi(\phi) \)) takes on discrete solutions, i.e. \( m = 0, \pm 1, \pm 2, \dots \)</p>
<p>Extending on this we may also derive that angular momentum is quantized.</p>
<blockquote>
<p>I&rsquo;m behind on class so the derivations for angular momentum and energy levels will not be typed out.</p>
</blockquote>
<p>$$ L_z = m\hbar \qquad[ m = 0, \pm 1, \pm2, \dots] $$</p>
<p>Much the same approach applies to the three dimensional case.
The solution is a bit of work to write out, so <a href="https://chem.libretexts.org/Courses/University_of_California_Davis/UCD_Chem_107B%3A_Physical_Chemistry_for_Life_Scientists/Chapters/4%3A_Quantum_Theory/4.10%3A_The_Schr%C3%B6dinger_Wave_Equation_for_the_Hydrogen_Atom">see this</a>.</p>
<p>The key takeaway is that by solving the Schrodinger equation we see quantization come out of the cracks naturally after we apply separation of variables i.e. \( \psi(r, \theta, \phi) = R(r) \Theta(\theta) \Phi(\phi) \)</p>
<ol>
<li>The \( \Phi \) equation</li>
</ol>
<p>$$ \Phi''(\phi) = 0m^2 \Phi(\phi) $$
Solving the \( \Phi \) equation gives us the solution \( \Phi(\phi) = e^{im\phi} \).
Since \( \Phi(\phi) \) must me periodic with period \( 2\pi \) we can use the Euler identity to find that \( m = 0, \pm 1, \pm2, \dots \) and that angular momentum is quantized i.e. \( L_z = m \hbar \)</p>
<ol start="2">
<li>The \( \Theta \) equation</li>
</ol>
<p>$$ \frac{1}{\sin(	\theta)} \frac{d}{d	\theta} (\sin	\theta \frac{d\Theta}{d\theta}) + (k - \frac{m^2}{\sin^2 \theta}) \Theta = 0 $$</p>
<p>This is a lot more difficult to solve and is called Legendre&rsquo;s equation.
It turns out that it has unique solutions for each \( k \) of form \( k = l(l+1) \qquad l \geq |m| \)</p>
<p>From this we get the result that particles with a wave function as given above has angular momentum</p>
<p>$$ L = \sqrt{l(l+1)} \hbar  $$</p>
<p>where \( l \) can be any integer greater than equal to 1, and for any given \( l \), \( m \) can take on integer values \( |m| \leq l \)</p>
<blockquote>
<p>It is sometimes useful to express this in vector form



 <img loading="lazy" src="img/ang_momentum_vector.png" alt="ang_momentum_vector"  />


The \( z \) component can take on \( 2l+1 \) values</p>
</blockquote>
<ol start="3">
<li>The \( R \) equation
$$ \frac{d^2}{dr^2} (rR) = \frac{2M}{\hbar} [U(r) + \frac{l(l+1)\hbar^2}{2Mr^2} - E] (rR) $$</li>
</ol>
<blockquote>
<ul>
<li>\( k = l(l+1) \)</li>
</ul>
</blockquote>
<p>This equation determines the form of the potential \( U(r) \).
Notice that it does not depend on \( m \), therefore for a given \( L \) we will find the same allowed energies for all \( 2l + 1 \) orientations; it is spherically symmetric. Therefore a level will always be <em>at least</em> \( 2l + 1 \) fold degenerate.
It turns out that the energies of the hydrogen atom are described by</p>
<p>$$ E = -\frac{m_e (ke^2)^2}{2\hbar^2} \frac{1}{n^2} $$</p>
<p>which can be simplified by recognizing the Rydberg energy term</p>
<p>$$ E = -\frac{E_R}{n^2} \qquad E_R = 13.6 eV $$</p>
<h5 id="wave-function">Wave Function</h5>
<p>$$ \phi_{nlm} = R_{nl}(r) \Theta_{lm}(\theta) e^{im\phi} $$</p>
<blockquote>
<p>Expressions for \( R \) and \( \Theta \) are given in tables</p>
</blockquote>
<p>


 <img loading="lazy" src="img/hydrogen_wavefns.png" alt="hydrogen_wavefns"  />

</p>
<blockquote>
<p>Solutions from textbook. I think these would be provided if applicable.</p>
</blockquote>
<h4 id="probability-density-functions">Probability Density Functions</h4>
<ul>
<li>We can derive probability density functions for different shells of the hydrogen atom. For example for the ground state \( P_{1s}(r) = 4\pi A^2 r^2 e^{-2r/a_B} \).
<ul>
<li>Since this is a probability density function, \( \int_{-\infty}^{\infty} P(r) = 1 \). This allows us to solve for the constants in that expression.</li>
</ul>
</li>
</ul>
<blockquote>
<p>


 <img loading="lazy" src="img/1s_prob_density.png" alt="1s_prob_density"  />


Notice maximum probability  at \( r = a_B \), which is the Bohr radius</p>
</blockquote>
<ul>
<li>More probability density functions can found with a google search.</li>
</ul>
<h4 id="hydrogen-like-atoms">Hydrogen-like atoms</h4>
<p>Hydrogen-like atoms can be approached in much the same way; whereas the potential of an electron in hydrogen is \( U = -ke^2/r \), it is \( U = -Zke^2/r \) in a hydrogen-like ion.</p>
<blockquote>
<p>\( Z \) is the atomic number</p>
</blockquote>
<p>Allowed energies of a hydrogen-like ion:</p>
<p>$$ E = -Z^2 \frac{E_R}{n^2} $$</p>
<p>And for finding the wave function apply much the same procedure as before but incorporate \( Z \), i.e. the parameter becomes</p>
<p>$$ \frac{\hbar^2}{m_e Z ke^2} = \frac{a_B}{Z} $$</p>
<p>For example the ground state of a hydrogen-like ion is given by</p>
<p>$$ \psi_{1s} = Ae^{-\frac{Zr}{a_B}} $$</p>
<h4 id="quantum-numbers-and-wrapping-up">Quantum Numbers, and wrapping up</h4>
<ul>
<li>\( n \in \{1, 2, 3, \dots \}\): principal quantum number, describes energy level of atom</li>
<li>\( l \in \{ 0, 1, 2, \dots, (n-1) \}\): azimuthal quantum number, describes angular momentum and sub-shell</li>
<li>\( m_l \in \{ -l, \dots, -1, 0, 1, \dots, l \} \): magnetic quantum number, describes energy levels available within a sub-shell</li>
<li>\( m_s \in \{ +1/2, -1/2 \}\): electron spin of the orbital</li>
<li>The \( n \)th level has degeneracy \( 2n^2 \). The 2 is caused by electron spin</li>
</ul>
<p><code>// following notes will be rather rough ... //</code></p>
<h3 id="electron-spin">Electron Spin</h3>
<blockquote>
<p>Electrons have a spin. But they don&rsquo;t really spin. Don&rsquo;t question it.</p>
</blockquote>
<p>$$ J = L + S $$</p>
<ul>
<li>\( J \) is the total angular momentum of the electron</li>
<li>\( L \) is the angular momentum of the electron</li>
<li>\( S \) is the spin of the electron</li>
</ul>
<blockquote>
<p>Think of the earth orbiting the sun; earth&rsquo;s spin is \( S \) and it&rsquo;s orbit is \( L \)</p>
</blockquote>
<ul>
<li>Spin is denoted via the spin quantum number, \( \pm \frac{1}{2} \)</li>
<li>We saw prior that \( L \) is quantized by \( L = \sqrt{l(l+1)} \hbar \)</li>
<li>Magnitude \( S = \sqrt{s(s+1) \hbar } = \frac{\sqrt{3}}{2} \hbar \): &ldquo;intrinsic angular momentum&rdquo;</li>
<li>degeneracy of <em>n</em> th level in hydrogen: \( 2n^2 \)</li>
</ul>
<p>The electron acts like a rotating electric charge.
A current \( i \) in a loop of area \( A \) in a magnetic field \( B \) experiences torque \( \Gamma \)</p>
<p>$$ \Gamma = iA \times B $$</p>
<p>or</p>
<p>$$ \Gamma = \mu \times B \qquad \mu = iA $$</p>
<p>The potential is is the negative of the work</p>
<p>$$ U = -\mu B cost cos \theta = -\vec{\mu} \cdot \vec{B}$$</p>
<p>We can define a ratio of \( \mu \) to \( L \) for the electron to to be</p>
<p>$$ \frac{\mu}{L} = \frac{e}{2m_e} \Rightarrow \vec{\mu} = -\frac{e}{2m_e}\vec{L} $$</p>
<p>Spin was demonstrated experimentally through the Zeeman experiment/Zeeman effect.
The basic intuition is that applying a magnetic field to an atom will induce a change in energy by \( -\vec{\mu} \cdot \vec{B} \) which depends on the orientation of \( \mu \).
This means that the energy will change for each of the \( 2l + 1 \) possible orientations; by applying a magnetic field we the degeneracy of the original energy level.</p>
<p>$$ \Delta E = (\frac{e}{2m_2}) L_z B $$</p>
<p>And if we extract out the <em>bohr magneton</em> constant \(  \mu_B = \frac{e\hbar}{2m_e} = 9.27 \times 10^-{24} A \cdot m^2 = 5.79 \times 10^{-5} eV/T \) this reduces to \( \Delta E = m\mu_B B \).
Since \( m \) can have \( 2l+1 \) values \( \longrightarrow \) separation of the energy levels is \( \mu_B B \).</p>
<p>Now, considering the spin up/down case, we expect the <em>total</em> spin to be proportional to the spin angular momentum, i.e.</p>
<p>$$ \vec{\mu}_{\text{spin}}= -\gamma \vec{S} = -\frac{e}{m_e}\vec{S} $$</p>
<blockquote>
<p>\( \gamma \) is the spin gyromagnetic ratio</p>
</blockquote>
<p>and the total moment of an electron is</p>
<p>$$ \vec{\mu_{\text{tot}}} = \vec{\mu_{\text{orb}}} + \vec{\mu_{\text{spin}}} = -\frac{e}{2m_e}(\vec{L} + 2\vec{S}) $$</p>
<p>armed with the above we can calculate the <em>anomalous</em> Zeeman effect (where the spin does contribute) to find that the separation of levels is</p>
<p>$$\Delta E 2\mu_B B $$</p>
<p>which is twice the value predicted for the normal effect (which makes sense since the \( \gamma \)) is twice the orbital ratio.</p>
<h3 id="multi-electron-atoms-pauli-principle-and-periodic-table">Multi-Electron Atoms, Pauli Principle, and Periodic Table</h3>
<ul>
<li>can usually use IPA (independent particle assumption) when working with more than one electron</li>
<li>need to find the IPA potential energy for electrons</li>
</ul>
<p>$$ U(r) = -Z_{eff}(r) \frac{ke^2}{r} $$</p>
<p>$$ \begin{cases}
Z_{eff}(r) \approx Z &amp; r \text{inside all other electrons} \\
Z_eff \approx 1 &amp; r \text{outside all other electrons} \\
\end{cases} $$</p>
<blockquote>
<p>Can model it as a superposition of two cases depending on distance from other electrons/nucleus</p>
</blockquote>
<p>\( U(r) \) is pretty similar to the hydrogen potential function we know and love so we can use that as a starting off point.</p>
<ul>
<li>Each level has degeneracy of at least \( 2(2l+1) \)</li>
</ul>
<hr>
<p><em>Define</em>: Pauli Exclusion Principle</p>
<p>&ldquo;No two electrons in a quantum system can occupy the same quantum state&rdquo;</p>
<hr>
<blockquote>
<p>anything with full subshells (i.e. all paired electrons) will have spin 0; non-full subshells e.g. alkali metals and halogens will have spin \( \frac{\sqrt{3}}{2} \).
For getting the angular momentum just take the \( l \) of the valence electron and plug it into \( L = \sqrt{l(l+1)} \hbar \)</p>
</blockquote>
<h3 id="molecules-and-bonding">Molecules and Bonding</h3>
<ul>
<li>
<p>\( R_o \) gives the bond length; distance between nuclei</p>
</li>
<li>
<p>\( B (eV) \) gives the bond energy; energy needed to seperate the bond</p>
</li>
<li>
<p>diatomic molecules are bonded together really really hard</p>
</li>
<li>
<p>noble gases interact very weakly with other atoms (recall gauss&rsquo;s law) and have really high energies for their excited states</p>
</li>
<li>
<p>ionic bonding; electron transfer causing a strong electrostatic attraction; creates electric dipole \( p = qd \)</p>
</li>
<li>
<p>covalent bonding;  sharing of electron</p>
</li>
<li>
<p>mixed bonds; a little of both &ndash; most bonds are a little bit of both. None are completely ionic and a few are completely covalent (i.e. \( H_2, O_2 \))</p>
</li>
<li>
<p>bond strengths can be estimated using the expression for electrostatic potential \( U = \frac{-ke^2}{R_o} \)</p>
</li>
<li>
<p>energy released in a chem. reaction depends on the bond energies; i.e. \( \Delta E = \sum_{i=1}^{n} \Delta E_i \) each \( E_i \) denotes the bond energy to make/break a bond</p>
</li>
</ul>
<p>ionic bonds</p>
<ul>
<li>two step processes; 1) transfer of electron (costs energy) 2) electron capture (energy gain); electron affinity; i.e. for forming \( NaCl \) the energy cost \( \Delta E \) is 5.1eV - 3.6eV = 1.5eV where 5.1 is the ionization energy and -3.6 is the electron affinity
<ul>
<li>so why do bonds form even though it costs energy? Note energy changes depending on the distance between them
<ul>
<li>bonding will happen at a critical distance \( R_c = \frac{ke^2}{\Delta E} \)</li>
<li>can estimate the binding energy \( B \approx \frac{ke^2}{R_o} - \Delta E \)</li>
</ul>
</li>
</ul>
</li>
<li>define <strong>valence</strong>: no. of electrons an atom gains/loses in forming a molecule.</li>
<li>a lot of atom behaviour depends on the valence of the atom</li>
</ul>
<p>covalent bonds</p>
<ul>
<li>must solve Schrodinger equation for the covalent bond
<ul>
<li>i am not going to be doing that now&hellip;</li>
</ul>
</li>
<li></li>
</ul>
<h2 id="esc204-praxis-iii">ESC204: Praxis III</h2>
<p><code>// coming ... maybe? //</code></p>
<h2 id="tep327-engineering-and-law">TEP327: Engineering and Law</h2>
<blockquote>
<p>** &ldquo;It depends&rdquo; **</p>
</blockquote>
<p>Why do engineers need to know about law?</p>
<ul>
<li>Engineering ends up stepping across a lot of jurisdictions, so we kinda need to know about it.</li>
</ul>
<p>Law is &hellip;</p>
<ul>
<li>
<p>very territorial: must note jurisdiction</p>
<ul>
<li>Must develop intuition for jurisdiction (See <a href="#resources">section 91</a>)</li>
<li>Generally:
<ul>
<li>Things outside of Canada &amp; maintained <em>across</em> Canada &ndash; Federal</li>
<li>Things that are managed within Canada and aren&rsquo;t necessarily consistent (local effect) &ndash; Provincial</li>
<li>Things that Provincial gov. doesn&rsquo;t want to deal with &ndash; Municipal (via mandate)</li>
<li>And for everything &ndash; consider case law, etc.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>always dependent on context and subject to interpretation; the law&rsquo;s wording can stay constant but the interpretation can change over time</p>
<ul>
<li>There are standard methods of interpretation (Living tree vs originalist, strict literal vs commercial reality/intentions)</li>
<li>Canada is bijural (both civil and common law)
<ul>
<li>Note difference between public (refers to society as a whole) and private law (individual interactions)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>professional engineers are bound by their administrative body</p>
<ul>
<li>operates under assumption that <em>only</em> engineers can truly understand how engineers should best act
<ul>
<li>part of it regulates that professional engineers should &ldquo;be of good character&rdquo;</li>
<li>code of ethics is enforceable
<ul>
<li>can be enforced by law because engineers are bound by professional engineers act (Ontario) or whatever it is in their governing body</li>
<li>it is intentionally and often vague</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="case-briefing">Case Briefing</h3>
<ol>
<li>Facts:</li>
</ol>
<blockquote>
<p>Note that with with law we first enter the facts and evidence, <em>then</em> apply the relevant law to it.</p>
</blockquote>
<ol start="2">
<li>Issues</li>
<li>Rule/Ratio</li>
<li>Reasoning</li>
<li>Holding</li>
</ol>
<ul>
<li>patents can have classifications (which will usually be given in patent search results)</li>
<li>can also search classifications and then filter by that.</li>
</ul>
<h3 id="resources">Resources</h3>
<ul>
<li><a href="https://laws-lois.justice.gc.ca/eng/const/page-3.html">Canadian Distribution of Legislative Powers</a></li>
<li><a href="https://www.canlii.org/en/">Canlii</a></li>
<li><a href="https://www.ontario.ca/laws/statute/90p28">Professional Engineers Act, R.S.O</a></li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://en.wikipedia.org/wiki/Markov_chain">https://en.wikipedia.org/wiki/Markov_chain</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://en.wikipedia.org/wiki/Linear_time-invariant_system">https://en.wikipedia.org/wiki/Linear_time-invariant_system</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>at least, not yet?&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

</article>


        </section>
    </div>
<div class="footer">
    
    
    <div class="footer-links">
        <a href="https://github.com/ihasdapie">GitHub</a> &nbsp;
        <a href="https://github.com/ihasdapie/bettermotherfuckinghugowebsite/">Theme</a> &nbsp;
    </div>
    

    
    
    <div class="copyright">© 2022 — Brian Chen — All rights reserved.</div>
    
</div>

</body>

</html>
